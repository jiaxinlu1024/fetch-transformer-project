{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924bfa8d",
   "metadata": {},
   "source": [
    "# Sentence Transformers & Multi-Task Learning Take Home Exercise\n",
    "---\n",
    "\n",
    "# Task 1: Sentence Transformer Implementation\n",
    "\n",
    "Hi! So glad you’re here to check out my work. I really enjoyed putting this model together. Let’s talk about the choices I made before we get to the code!\n",
    "\n",
    "---\n",
    "\n",
    "# 1 Intoduction\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Why BERT Was My Choice\n",
    "\n",
    "I picked **BERT** as the core of my model, it's a perfect starting point. Here’s why:\n",
    "\n",
    "- **It perfectly fits the downstream tasks**:  \n",
    "  I needed a model that could handle sentence classification and named entity recognition. BERT’s got `[CLS]` or mean embedding token for sentence-level tasks and per-token embeddings for task like NER. \n",
    "  \n",
    "\n",
    "## 1.2 Choices regarding the model architecture outside of the transformer backbone.\n",
    "\n",
    "While BERT provides a solid foundation, several modifications were made to tailor it to this specific task.\n",
    "\n",
    "### **1.2.1 Three Embedding Methods**  \n",
    "I implemented three ways to extract embeddings:  \n",
    "\n",
    "1. **CLS token embedding** – Uses the `[CLS]` token for sentence representation.  \n",
    "2. **Mean pooling** – Averages all token embeddings to capture overall sentence meaning.  \n",
    "3. **Per-token embeddings** – Provides embeddings for each token (important for NER).  \n",
    "\n",
    "### **1.2.2 Attention Masking**  \n",
    "- Sentences in a batch vary in length, so **padding is needed**.  \n",
    "- Without masking, **padding tokens could distort embeddings**.  \n",
    "- **Attention masks** were used to ensure padding tokens were ignored in calculations.  \n",
    "\n",
    "### **1.2.3 Tokenization Strategy**  \n",
    "- I used **WordPiece** tokenizer to remain consistent with BERT’s pre-trained model.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1.3. Validation and Testing**  \n",
    "\n",
    "To confirm that my model was working as intended, I followed two key steps.  \n",
    "\n",
    "### **1.3.1 Matching My Model with Pre-Trained BERT**  \n",
    "Since I copied weights from `bert-base-uncased`, from the same input text I checked the output of my model and the pre-trained model. If they matched, it meant my implementation was correct.  \n",
    "\n",
    "### **1.3.2 Cosine Similarity Testing**  \n",
    "- **Three sentences were tested:**  \n",
    "  - Two similar sentences.  \n",
    "  - One unrelated sentence.  \n",
    "- **Expected results:**  \n",
    "  - **Higher similarity** for related sentences.  \n",
    "  - **Lower similarity** for unrelated sentences.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1.4. Summary of Key Decisions**  \n",
    "\n",
    "### **Model Selection**  \n",
    "- Chose **BERT (`bert-base-uncased`)** for its strong performance in **classification and NER**.  \n",
    "- Used a **pre-trained tokenizer** to ensure consistency with BERT’s input expectations.  \n",
    "\n",
    "### **Architectural Adjustments**  \n",
    "- **Used mean pooling** instead of `[CLS]` for better sentence representations.  \n",
    "- **Implemented attention masking** to exclude padding tokens.  \n",
    "- **Made embedding normalization optional** to support different tasks.  \n",
    "\n",
    "### **Validation Approach**  \n",
    "- **Compared embeddings** with pre-trained BERT to ensure correct weight initialization.  \n",
    "- **Ran similarity tests** to verify that embeddings captured semantic relationships correctly.  \n",
    "\n",
    "---\n",
    "\n",
    "# 2 Implementation SentenceTransformer of  Using BERT\n",
    "\n",
    "\n",
    "### **Key Features**\n",
    "- Uses **BERT (`bert-base-uncased`)** as a backbone.\n",
    "- Supports **three types of embeddings**:\n",
    "  - `[CLS]` token embedding for classification.\n",
    "  - **Mean pooling** for sentence representation.\n",
    "  - **Full token embeddings** for NER and other sequence-based tasks.\n",
    "- Implements a **custom Transformer model** with weights copied from a pre-trained BERT model.\n",
    "- **Validates the model** by comparing its outputs with those from `bert-base-uncased`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c56ab",
   "metadata": {},
   "source": [
    "## **2.1. Import Libraries**\n",
    "We first import necessary libraries for building and testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef49f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f511d",
   "metadata": {},
   "source": [
    "## **2.2 Define Multi-Head Self-Attention**\n",
    "This module implements **multi-head self-attention**, a core component of Transformers.  \n",
    "It enables the model to **focus on different parts of a sentence simultaneously**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24ef0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-Head Self-Attention Layer\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size)  \n",
    "        self.key_proj = nn.Linear(embed_size, embed_size)    \n",
    "        self.value_proj = nn.Linear(embed_size, embed_size)  \n",
    "        self.output_proj = nn.Linear(embed_size, embed_size) \n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        batch_size = queries.shape[0]\n",
    "\n",
    "        Q = self.query_proj(queries)\n",
    "        K = self.key_proj(keys)\n",
    "        V = self.value_proj(values)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scaling_factor = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / scaling_factor\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_mask_add = mask * -10000.0  \n",
    "            attention_scores = attention_scores + attention_mask_add\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        \n",
    "        return self.output_proj(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffa024",
   "metadata": {},
   "source": [
    "## **2.3 Define Transformer Encoder Layer**\n",
    "Each **encoder layer** contains:\n",
    "- **Multi-head self-attention**\n",
    "- **Layer normalization**\n",
    "- **Feed-forward neural network**\n",
    "- **Dropout for regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b421db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, hidden_dim, dropout=0.1, layer_norm_eps=1e-12):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(embed_size, eps=layer_norm_eps)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638fc7b8",
   "metadata": {},
   "source": [
    "## **2.4 Define Sentence Transformer Model**\n",
    "This custom transformer:\n",
    "- Supports **CLS, Mean Pooling, and Token Embeddings**.\n",
    "- Uses **BERT-like embeddings** for token, position, and token type.\n",
    "- Stacks multiple **Transformer Encoder layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0ea8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_length, \n",
    "                 pooling=\"cls\", output_mode=\"sentence\", layer_norm_eps=1e-12):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.pooling = pooling          # \"cls\" or \"mean\" for sentence embeddings\n",
    "        self.output_mode = output_mode  # \"sentence\" or \"token\"\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embeddings = nn.Embedding(max_length, embed_size)\n",
    "        self.token_type_embeddings = nn.Embedding(2, embed_size)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            Encoder(embed_size, num_heads, hidden_dim, layer_norm_eps=layer_norm_eps) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.embed_layer_norm = nn.LayerNorm(embed_size, eps=layer_norm_eps)\n",
    "        self.embed_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        token_embed = self.token_embeddings(input_ids)\n",
    "        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        position_embed = self.position_embeddings(position_ids)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embed = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        x = token_embed + position_embed + token_type_embed\n",
    "        x = self.embed_layer_norm(x)\n",
    "        x = self.embed_dropout(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            mask = (attention_mask == 0).unsqueeze(1).unsqueeze(2).to(x.device)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        if self.output_mode == \"sentence\":\n",
    "            if self.pooling == \"cls\":\n",
    "                return x[:, 0, :]  \n",
    "            elif self.pooling == \"mean\":\n",
    "                return x.mean(dim=1)  \n",
    "        elif self.output_mode == \"token\":\n",
    "            return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5b5da",
   "metadata": {},
   "source": [
    "## **2.5 Function to Initialize Custom Model**\n",
    "This function:\n",
    "- **Extracts hyperparameters** from the pre-trained BERT model.\n",
    "- **Initializes a custom Transformer model** with the same architecture.\n",
    "- **Copies weights** from BERT to the custom model for a head start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "260a866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_custom_model(pretrained_model, model_type=\"sentence\", pooling=\"cls\", output_mode=\"sentence\",\n",
    "                            num_classes=2, num_entity_labels=17):\n",
    "    \"\"\"\n",
    "    Initialize a custom model (either SentenceTransformer or MultiTaskSentenceTransformer) and copy weights \n",
    "    from a pretrained BERT model.\n",
    "    \"\"\"\n",
    "\n",
    "    # **Function to Copy Weights from Pretrained BERT Model**\n",
    "    def copy_pretrained_weights(custom_model, pretrained_model):\n",
    "        \"\"\"Copy weights from a pretrained BERT model to the custom SentenceTransformer or MultiTaskSentenceTransformer.\"\"\"\n",
    "        # Copy embedding weights\n",
    "        custom_model.token_embeddings.weight.data.copy_(pretrained_model.embeddings.word_embeddings.weight.data)\n",
    "        custom_model.position_embeddings.weight.data.copy_(pretrained_model.embeddings.position_embeddings.weight.data)\n",
    "        custom_model.token_type_embeddings.weight.data.copy_(\n",
    "            pretrained_model.embeddings.token_type_embeddings.weight.data)\n",
    "        custom_model.embed_layer_norm.weight.data.copy_(pretrained_model.embeddings.LayerNorm.weight.data)\n",
    "        custom_model.embed_layer_norm.bias.data.copy_(pretrained_model.embeddings.LayerNorm.bias.data)\n",
    "\n",
    "        # Copy weights for each encoder layer\n",
    "        for custom_layer, pretrained_layer in zip(custom_model.encoder_layers, pretrained_model.encoder.layer):\n",
    "            # Self-attention projections\n",
    "            custom_layer.self_attention.query_proj.weight.data.copy_(pretrained_layer.attention.self.query.weight.data)\n",
    "            custom_layer.self_attention.query_proj.bias.data.copy_(pretrained_layer.attention.self.query.bias.data)\n",
    "            custom_layer.self_attention.key_proj.weight.data.copy_(pretrained_layer.attention.self.key.weight.data)\n",
    "            custom_layer.self_attention.key_proj.bias.data.copy_(pretrained_layer.attention.self.key.bias.data)\n",
    "            custom_layer.self_attention.value_proj.weight.data.copy_(pretrained_layer.attention.self.value.weight.data)\n",
    "            custom_layer.self_attention.value_proj.bias.data.copy_(pretrained_layer.attention.self.value.bias.data)\n",
    "            custom_layer.self_attention.output_proj.weight.data.copy_(\n",
    "                pretrained_layer.attention.output.dense.weight.data)\n",
    "            custom_layer.self_attention.output_proj.bias.data.copy_(pretrained_layer.attention.output.dense.bias.data)\n",
    "\n",
    "            # Feed-forward network\n",
    "            custom_layer.feed_forward[0].weight.data.copy_(pretrained_layer.intermediate.dense.weight.data)\n",
    "            custom_layer.feed_forward[0].bias.data.copy_(pretrained_layer.intermediate.dense.bias.data)\n",
    "            custom_layer.feed_forward[2].weight.data.copy_(pretrained_layer.output.dense.weight.data)\n",
    "            custom_layer.feed_forward[2].bias.data.copy_(pretrained_layer.output.dense.bias.data)\n",
    "\n",
    "            # Layer normalization\n",
    "            custom_layer.norm1.weight.data.copy_(pretrained_layer.attention.output.LayerNorm.weight.data)\n",
    "            custom_layer.norm1.bias.data.copy_(pretrained_layer.attention.output.LayerNorm.bias.data)\n",
    "            custom_layer.norm2.weight.data.copy_(pretrained_layer.output.LayerNorm.weight.data)\n",
    "            custom_layer.norm2.bias.data.copy_(pretrained_layer.output.LayerNorm.bias.data)\n",
    "        print(\"Weights successfully copied from pretrained BERT to custom model!\")\n",
    "\n",
    "    # **Extract Hyperparameters from Pretrained Model**\n",
    "    config = pretrained_model.config\n",
    "    vocab_size = config.vocab_size\n",
    "    embed_size = config.hidden_size\n",
    "    num_heads = config.num_attention_heads\n",
    "    hidden_dim = config.intermediate_size\n",
    "    num_layers = config.num_hidden_layers\n",
    "    max_length = config.max_position_embeddings\n",
    "    layer_norm_eps = config.layer_norm_eps  # Use BERT's epsilon\n",
    "\n",
    "    print(f\"Vocab Size: {vocab_size}\")\n",
    "    print(f\"Embedding Size: {embed_size}\")\n",
    "    print(f\"Number of Heads: {num_heads}\")\n",
    "    print(f\"Hidden Dimension: {hidden_dim}\")\n",
    "    print(f\"Number of Layers: {num_layers}\")\n",
    "    print(f\"Max Position Embeddings: {max_length}\")\n",
    "    print(f\"Layer Norm Epsilon: {layer_norm_eps}\")\n",
    "\n",
    "    # **Initialize Custom Model Based on Model Type** , sentence is for Task 1 ,and multitask for task 2 \n",
    "    if model_type == \"sentence\":\n",
    "        custom_model = SentenceTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            max_length=max_length,\n",
    "            pooling=pooling,\n",
    "            output_mode=output_mode,\n",
    "            layer_norm_eps=layer_norm_eps\n",
    "        )\n",
    "    elif model_type == \"multitask\":\n",
    "        custom_model = MultiTaskSentenceTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            max_length=max_length,\n",
    "            num_classes=num_classes,\n",
    "            num_entity_labels=num_entity_labels,\n",
    "            pooling=pooling,\n",
    "            layer_norm_eps=layer_norm_eps\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}. Use 'sentence' or 'multitask'.\")\n",
    "\n",
    "    # **Copy Weights from Pretrained Model**\n",
    "    copy_pretrained_weights(custom_model, pretrained_model)\n",
    "\n",
    "    print(\"\\nCustom model initialized and weights copied successfully.\")\n",
    "\n",
    "    return custom_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2205dd5",
   "metadata": {},
   "source": [
    "## **2.6 Sentence Embeddings and Similarity Calculation**\n",
    "This function:\n",
    "- Generates sentence embeddings using **CLS, Mean Pooling, and Full Token Embeddings**.\n",
    "- Computes **cosine similarity** to check if semantically similar sentences are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05189e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_emb_predicting(custom_model, pretrained_model, tokenizer, sentences):\n",
    "    \"\"\"\n",
    "    Demonstrates embedding extraction from both a custom model and a pretrained BERT model.\n",
    "    Compares CLS, mean, and token embeddings using cosine similarity.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract CLS embeddings\n",
    "        custom_model.output_mode = \"sentence\"\n",
    "        custom_model.pooling = \"cls\"\n",
    "        custom_cls_embeddings = custom_model(tokens[\"input_ids\"], tokens[\"attention_mask\"], tokens[\"token_type_ids\"])\n",
    "        bert_outputs = pretrained_model(**tokens)\n",
    "        bert_cls_embeddings = bert_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Extract mean embeddings\n",
    "        custom_model.pooling = \"mean\"\n",
    "        custom_mean_embeddings = custom_model(tokens[\"input_ids\"], tokens[\"attention_mask\"], tokens[\"token_type_ids\"])\n",
    "        bert_token_embeddings = bert_outputs.last_hidden_state\n",
    "        bert_mean_embeddings = bert_token_embeddings.mean(dim=1)\n",
    "\n",
    "        # Extract token embeddings (full sequence)\n",
    "        custom_model.output_mode = \"token\"\n",
    "        custom_token_embeddings = custom_model(tokens[\"input_ids\"], tokens[\"attention_mask\"], tokens[\"token_type_ids\"])\n",
    "        bert_token_embeddings = bert_outputs.last_hidden_state\n",
    "\n",
    "    # Normalize embeddings for cosine similarity comparison\n",
    "    custom_cls_embeddings = F.normalize(custom_cls_embeddings, p=2, dim=-1)\n",
    "    custom_mean_embeddings = F.normalize(custom_mean_embeddings, p=2, dim=-1)\n",
    "    bert_cls_embeddings = F.normalize(bert_cls_embeddings, p=2, dim=-1)\n",
    "    bert_mean_embeddings = F.normalize(bert_mean_embeddings, p=2, dim=-1)\n",
    "\n",
    "    def compute_similarity(emb1, emb2):\n",
    "        \"\"\"Computes cosine similarity between two embeddings.\"\"\"\n",
    "        return F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "\n",
    "    # Compute similarities for CLS and mean embeddings\n",
    "    cosine_sim_custom_cls_1_2 = compute_similarity(custom_cls_embeddings[0], custom_cls_embeddings[1])\n",
    "    cosine_sim_custom_cls_2_3 = compute_similarity(custom_cls_embeddings[1], custom_cls_embeddings[2])\n",
    "    cosine_sim_custom_mean_1_2 = compute_similarity(custom_mean_embeddings[0], custom_mean_embeddings[1])\n",
    "    cosine_sim_custom_mean_2_3 = compute_similarity(custom_mean_embeddings[1], custom_mean_embeddings[2])\n",
    "\n",
    "    cosine_sim_bert_cls_1_2 = compute_similarity(bert_cls_embeddings[0], bert_cls_embeddings[1])\n",
    "    cosine_sim_bert_cls_2_3 = compute_similarity(bert_cls_embeddings[1], bert_cls_embeddings[2])\n",
    "    cosine_sim_bert_mean_1_2 = compute_similarity(bert_mean_embeddings[0], bert_mean_embeddings[1])\n",
    "    cosine_sim_bert_mean_2_3 = compute_similarity(bert_mean_embeddings[1], bert_mean_embeddings[2])\n",
    "\n",
    "    # Display embedding demonstrations\n",
    "    print(\"\\n **Embedding Demonstrations**\")\n",
    "\n",
    "    # Display CLS embeddings\n",
    "    print(\"\\n--- CLS Embeddings ---\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        print(f\"Sentence {i + 1}: \\\"{sentence}\\\"\")\n",
    "        print(f\"Custom Model CLS Embedding: {custom_cls_embeddings[i].tolist()[:5]} ...\")  # Print first 5 dimensions\n",
    "        print(f\"Pretrained BERT CLS Embedding: {bert_cls_embeddings[i].tolist()[:5]} ...\\n\")\n",
    "\n",
    "    # Display mean embeddings\n",
    "    print(\"\\n--- Mean Pooling Embeddings ---\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        print(f\"Sentence {i + 1}: \\\"{sentence}\\\"\")\n",
    "        print(f\"Custom Model Mean Embedding: {custom_mean_embeddings[i].tolist()[:5]} ...\")\n",
    "        print(f\"Pretrained BERT Mean Embedding: {bert_mean_embeddings[i].tolist()[:5]} ...\\n\")\n",
    "\n",
    "    # Display cosine similarity results\n",
    "    print(\"\\n **Cosine Similarity Results**\")\n",
    "    print(\"\\n--- Custom Model ---\")\n",
    "    print(f\"Custom Model CLS Similarity (Sentence 1 & 2): {cosine_sim_custom_cls_1_2:.4f}\")\n",
    "    print(f\"Custom Model CLS Similarity (Sentence 2 & 3): {cosine_sim_custom_cls_2_3:.4f}\")\n",
    "    print(f\"Custom Model Mean Similarity (Sentence 1 & 2): {cosine_sim_custom_mean_1_2:.4f}\")\n",
    "    print(f\"Custom Model Mean Similarity (Sentence 2 & 3): {cosine_sim_custom_mean_2_3:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Pretrained BERT ---\")\n",
    "    print(f\"Pretrained BERT CLS Similarity (Sentence 1 & 2): {cosine_sim_bert_cls_1_2:.4f}\")\n",
    "    print(f\"Pretrained BERT CLS Similarity (Sentence 2 & 3): {cosine_sim_bert_cls_2_3:.4f}\")\n",
    "    print(f\"Pretrained BERT Mean Similarity (Sentence 1 & 2): {cosine_sim_bert_mean_1_2:.4f}\")\n",
    "    print(f\"Pretrained BERT Mean Similarity (Sentence 2 & 3): {cosine_sim_bert_mean_2_3:.4f}\")\n",
    "\n",
    "    # Display token embeddings\n",
    "    print(\"\\n--- Token (Full Sequence) Embeddings ---\")\n",
    "    print(f\"Custom Model Token Embeddings Shape: {custom_token_embeddings.shape}\")\n",
    "    print(f\"Pretrained BERT Token Embeddings Shape: {bert_token_embeddings.shape}\")\n",
    "    print(\n",
    "        f\"\\nCustom Model Token Embedding (First 5 Tokens): {custom_token_embeddings[:, :5, :]} ...\")\n",
    "    print(\n",
    "        f\"Pretrained BERT Token Embedding (First 5 Tokens): {bert_token_embeddings[:, :5, :]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00519e2a",
   "metadata": {},
   "source": [
    "## **2.7 Main Execution**\n",
    "1. Load the **pretrained BERT model** and tokenizer.\n",
    "2. Initialize the **custom sentence transformer**.\n",
    "3. **Compare embeddings and similarities** using sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69f3cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30522\n",
      "Embedding Size: 768\n",
      "Number of Heads: 12\n",
      "Hidden Dimension: 3072\n",
      "Number of Layers: 12\n",
      "Max Position Embeddings: 512\n",
      "Layer Norm Epsilon: 1e-12\n",
      "Weights successfully copied from pretrained BERT to custom model!\n",
      "\n",
      "Custom model initialized and weights copied successfully.\n",
      "\n",
      " **Embedding Demonstrations**\n",
      "\n",
      "--- CLS Embeddings ---\n",
      "Sentence 1: \"Fetch Rewards is a great app!\"\n",
      "Custom Model CLS Embedding: [0.009375251829624176, -0.003474772907793522, 0.0010710387723520398, -0.0010895710438489914, -0.035332974046468735] ...\n",
      "Pretrained BERT CLS Embedding: [0.009375232271850109, -0.003474779427051544, 0.0010710282949730754, -0.0010895652230829, -0.035332970321178436] ...\n",
      "\n",
      "Sentence 2: \"I love eating apples.\"\n",
      "Custom Model CLS Embedding: [0.008211217820644379, 0.016353486105799675, -0.030486492440104485, -0.028192920610308647, -0.03595130145549774] ...\n",
      "Pretrained BERT CLS Embedding: [0.008211224339902401, 0.016353493556380272, -0.030486511066555977, -0.028192922472953796, -0.03595130890607834] ...\n",
      "\n",
      "Sentence 3: \"I do enjoy oranges!\"\n",
      "Custom Model CLS Embedding: [0.009734142571687698, 0.008232454769313335, -0.013198229484260082, -0.015298618003726006, -0.018957896158099174] ...\n",
      "Pretrained BERT CLS Embedding: [0.009734134189784527, 0.00823244545608759, -0.01319824531674385, -0.015298612415790558, -0.018957896158099174] ...\n",
      "\n",
      "\n",
      "--- Mean Pooling Embeddings ---\n",
      "Sentence 1: \"Fetch Rewards is a great app!\"\n",
      "Custom Model Mean Embedding: [0.016200730577111244, -0.03647976368665695, 0.03968100622296333, 0.015346313826739788, 0.018839120864868164] ...\n",
      "Pretrained BERT Mean Embedding: [0.016200687736272812, -0.036479800939559937, 0.03968101367354393, 0.015346325002610683, 0.018839120864868164] ...\n",
      "\n",
      "Sentence 2: \"I love eating apples.\"\n",
      "Custom Model Mean Embedding: [0.043881937861442566, 0.05217462405562401, -0.04034167155623436, -0.015764828771352768, 0.00732442969456315] ...\n",
      "Pretrained BERT Mean Embedding: [0.04388194903731346, 0.0521746389567852, -0.040341656655073166, -0.015764817595481873, 0.007324416656047106] ...\n",
      "\n",
      "Sentence 3: \"I do enjoy oranges!\"\n",
      "Custom Model Mean Embedding: [0.018066896125674248, 0.02168177254498005, -0.00838460586965084, -0.012830652296543121, 0.021313440054655075] ...\n",
      "Pretrained BERT Mean Embedding: [0.018066881224513054, 0.021681755781173706, -0.008384610526263714, -0.012830672785639763, 0.021313466131687164] ...\n",
      "\n",
      "\n",
      " **Cosine Similarity Results**\n",
      "\n",
      "--- Custom Model ---\n",
      "Custom Model CLS Similarity (Sentence 1 & 2): 0.8989\n",
      "Custom Model CLS Similarity (Sentence 2 & 3): 0.9466\n",
      "Custom Model Mean Similarity (Sentence 1 & 2): 0.5787\n",
      "Custom Model Mean Similarity (Sentence 2 & 3): 0.8368\n",
      "\n",
      "--- Pretrained BERT ---\n",
      "Pretrained BERT CLS Similarity (Sentence 1 & 2): 0.8989\n",
      "Pretrained BERT CLS Similarity (Sentence 2 & 3): 0.9466\n",
      "Pretrained BERT Mean Similarity (Sentence 1 & 2): 0.5787\n",
      "Pretrained BERT Mean Similarity (Sentence 2 & 3): 0.8368\n",
      "\n",
      "--- Token (Full Sequence) Embeddings ---\n",
      "Custom Model Token Embeddings Shape: torch.Size([3, 9, 768])\n",
      "Pretrained BERT Token Embeddings Shape: torch.Size([3, 9, 768])\n",
      "\n",
      "Custom Model Token Embedding (First 5 Tokens): tensor([[[ 1.3766e-01, -5.1020e-02,  1.5726e-02,  ..., -2.6633e-01,\n",
      "           1.7043e-02,  1.9203e-01],\n",
      "         [ 3.6954e-01, -7.0537e-01,  8.1871e-01,  ...,  1.3754e-01,\n",
      "           1.3798e-05,  5.9257e-01],\n",
      "         [ 4.5924e-01, -7.3651e-01,  6.5095e-01,  ..., -5.3360e-02,\n",
      "           3.4620e-02, -3.0168e-01],\n",
      "         [-5.9493e-02, -3.9310e-02,  2.9292e-01,  ..., -3.6405e-01,\n",
      "          -3.5053e-02,  4.3147e-01],\n",
      "         [-3.7823e-01, -5.3785e-01,  4.2798e-01,  ..., -1.5181e-01,\n",
      "           4.8671e-01,  5.9132e-01]],\n",
      "\n",
      "        [[ 1.2214e-01,  2.4325e-01, -4.5346e-01,  ..., -5.0201e-02,\n",
      "           2.9040e-01,  4.9338e-01],\n",
      "         [ 4.2972e-01,  1.5153e-01, -5.9048e-01,  ..., -6.6990e-02,\n",
      "           6.7660e-01,  3.1781e-01],\n",
      "         [ 8.4696e-01,  7.6612e-01,  1.5102e-01,  ...,  2.2629e-01,\n",
      "           2.8435e-01,  2.8353e-03],\n",
      "         [ 1.8906e-01,  9.7573e-01, -4.0476e-01,  ...,  2.2605e-01,\n",
      "          -2.6526e-01, -3.9075e-01],\n",
      "         [-1.5145e-01,  1.0999e+00, -7.4980e-01,  ...,  1.6332e-01,\n",
      "          -7.6592e-01, -1.3349e+00]],\n",
      "\n",
      "        [[ 1.4195e-01,  1.2005e-01, -1.9247e-01,  ..., -4.7228e-02,\n",
      "           3.2379e-01,  4.6842e-01],\n",
      "         [ 3.1677e-01,  1.3371e-01, -3.5884e-01,  ...,  5.7045e-02,\n",
      "           6.1248e-01,  1.6059e-01],\n",
      "         [ 1.1312e-01,  1.7985e-01, -3.0358e-01,  ..., -3.0740e-01,\n",
      "           3.5643e-01, -5.0964e-01],\n",
      "         [ 1.7417e-01,  7.6867e-01,  3.1378e-01,  ...,  3.8574e-01,\n",
      "           8.6986e-03, -5.3447e-02],\n",
      "         [-4.5033e-01,  3.2155e-01, -1.4400e-01,  ...,  1.2990e-01,\n",
      "          -5.7440e-01, -7.0617e-01]]]) ...\n",
      "Pretrained BERT Token Embedding (First 5 Tokens): tensor([[[ 1.3766e-01, -5.1020e-02,  1.5726e-02,  ..., -2.6633e-01,\n",
      "           1.7044e-02,  1.9203e-01],\n",
      "         [ 3.6953e-01, -7.0537e-01,  8.1871e-01,  ...,  1.3754e-01,\n",
      "           1.3221e-05,  5.9257e-01],\n",
      "         [ 4.5924e-01, -7.3651e-01,  6.5095e-01,  ..., -5.3361e-02,\n",
      "           3.4620e-02, -3.0168e-01],\n",
      "         [-5.9494e-02, -3.9311e-02,  2.9292e-01,  ..., -3.6405e-01,\n",
      "          -3.5053e-02,  4.3147e-01],\n",
      "         [-3.7823e-01, -5.3785e-01,  4.2798e-01,  ..., -1.5181e-01,\n",
      "           4.8671e-01,  5.9132e-01]],\n",
      "\n",
      "        [[ 1.2214e-01,  2.4325e-01, -4.5346e-01,  ..., -5.0201e-02,\n",
      "           2.9040e-01,  4.9338e-01],\n",
      "         [ 4.2972e-01,  1.5153e-01, -5.9048e-01,  ..., -6.6991e-02,\n",
      "           6.7660e-01,  3.1781e-01],\n",
      "         [ 8.4696e-01,  7.6612e-01,  1.5102e-01,  ...,  2.2629e-01,\n",
      "           2.8435e-01,  2.8353e-03],\n",
      "         [ 1.8906e-01,  9.7573e-01, -4.0476e-01,  ...,  2.2605e-01,\n",
      "          -2.6526e-01, -3.9075e-01],\n",
      "         [-1.5145e-01,  1.0999e+00, -7.4980e-01,  ...,  1.6332e-01,\n",
      "          -7.6592e-01, -1.3349e+00]],\n",
      "\n",
      "        [[ 1.4195e-01,  1.2005e-01, -1.9247e-01,  ..., -4.7228e-02,\n",
      "           3.2379e-01,  4.6842e-01],\n",
      "         [ 3.1677e-01,  1.3371e-01, -3.5884e-01,  ...,  5.7045e-02,\n",
      "           6.1248e-01,  1.6059e-01],\n",
      "         [ 1.1312e-01,  1.7985e-01, -3.0358e-01,  ..., -3.0741e-01,\n",
      "           3.5643e-01, -5.0964e-01],\n",
      "         [ 1.7417e-01,  7.6867e-01,  3.1378e-01,  ...,  3.8574e-01,\n",
      "           8.6985e-03, -5.3447e-02],\n",
      "         [-4.5033e-01,  3.2155e-01, -1.4400e-01,  ...,  1.2990e-01,\n",
      "          -5.7440e-01, -7.0617e-01]]]) ...\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "custom_model = initialize_custom_model(pretrained_model)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "custom_model.eval()\n",
    "pretrained_model.eval()\n",
    "sentences = [\n",
    "    \"Fetch Rewards is a great app!\",\n",
    "    \"I love eating apples.\",\n",
    "    \"I do enjoy oranges!\"\n",
    "]\n",
    "demo_emb_predicting(custom_model, pretrained_model, tokenizer, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993eea5f",
   "metadata": {},
   "source": [
    "## 2.8 Analysis of Results\n",
    "The output compares embeddings and cosine similarities from our custom model with the pretrained BERT model for three sentences. CLS and mean pooling embeddings exhibit near-identical values (e.g., `0.0093752518` vs. `0.0093752322`), confirming successful weight transfer from BERT and accurate forward pass implementation. Cosine similarity scores align perfectly (e.g., 0.9466 for Sentences 2 and 3), reflecting the model’s ability to capture semantic proximity (e.g., higher similarity for Sentences 2 and 3 due to shared context). Token embeddings maintain consistency in shape (`[3, 9, 768]`) and values (e.g., `1.3798e-05` vs. `1.3221e-05`), validating token-level fidelity. These results demonstrate that the custom model effectively replicates BERT’s functionality, establishing a reliable foundation for downstream NLP tasks such as classification and NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7ee6e",
   "metadata": {},
   "source": [
    "## 2.9 Summary\n",
    "\n",
    "This implementation is a robust and versatile sentence transformer setup based on the BERT architecture:\n",
    "- **Data Handling**: Utilizes the pre-trained `BertTokenizer` to process input sentences consistently with `bert-base-uncased`, ensuring compatibility with the model's expectations.\n",
    "- **Model**: A custom `SentenceTransformer` with a BERT-like structure, incorporating multi-head self-attention, transformer encoder layers, and support for CLS token, mean pooling, and per-token embeddings, initialized with weights copied from the pre-trained BERT model.\n",
    "- **Validation**: Includes a comprehensive embedding comparison with the pre-trained BERT model and cosine similarity testing on sample sentences to verify correct implementation and semantic alignment.\n",
    "- **Usage**: Demonstrates the initialization process, embedding generation across different pooling strategies, and similarity analysis, providing a solid foundation for downstream tasks like classification and NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052da59",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Transformer Implementation\n",
    "\n",
    "Hi there! I’m thrilled to have you here to explore my multi-task transformer model. \n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 What’s This Task About.\n",
    "\n",
    "For Task 2, I built a model that tackles two distinct NLP tasks: **sentence classification** and **Named Entity Recognition (NER)**. The idea is to use a single transformer model to handle both, sharing knowledge across tasks to boost efficiency and performance. Here’s a quick rundown of the tasks:\n",
    "\n",
    "- **Sentence Classification**: Predicts a label for an entire sentence—like deciding if a movie review is \"positive\" or \"negative.\" It’s a sentence-level task.\n",
    "- **Named Entity Recognition (NER)**: Identifies and classifies entities (e.g., people, locations) in a sentence at the token level—like tagging \"New York\" as a location.\n",
    "\n",
    "Multi-task learning is perfect here because it lets the model learn shared linguistic patterns while customizing outputs for each task.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 How We’ll Construct the Model\n",
    "\n",
    "Here’s how I designed the model to handle both tasks simultaneously:\n",
    "\n",
    "### 1.2.1 Shared Backbone, Task-Specific Heads\n",
    "\n",
    "The model uses a **shared BERT backbone** to generate contextual embeddings, then splits into two lightweight heads:\n",
    "- **Classification Head**: A linear layer takes the sentence embedding (from `[CLS]` or mean pooling) and outputs logits for sentiment labels.\n",
    "- **NER Head**: Another linear layer takes the full sequence of token embeddings and outputs per-token logits for entity tags.\n",
    "\n",
    "This setup:\n",
    "- Reuses the transformer’s computations for efficiency.\n",
    "- Keeps task-specific predictions separate and tailored.\n",
    "\n",
    "### 1.2.2 Calculating Outputs for Both Tasks\n",
    "\n",
    "In the forward pass, the model **always computes both outputs**:\n",
    "- **Classification Logits**: For sentence-level predictions (e.g., positive/negative).\n",
    "- **NER Logits**: For token-level predictions (e.g., \"B-geo\", \"O\").\n",
    "\n",
    "This dual-output approach simplifies training and inference—I can use whichever output I need based on the task, without recomputing the backbone.\n",
    "\n",
    "### 1.2.3 Pooling Options for Classification\n",
    "\n",
    "For sentence classification, I implemented two pooling strategies:\n",
    "- **CLS Token Pooling**: Uses the `[CLS]` embedding—BERT’s built-in sentence representation.\n",
    "- **Mean Pooling**: Averages all token embeddings for a more holistic sentence view.\n",
    "\n",
    "After experimenting, **mean pooling often outperformed `[CLS]`**, so I made it an optional parameter you can toggle.\n",
    "\n",
    "\n",
    "## 1.3 Datasets We’re Using\n",
    "\n",
    "I worked with two datasets to train and test the model:\n",
    "- **Classification Dataset**: IMDB Movie reviews labeled \"positive\" or \"negative.\" \n",
    "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "- **NER Dataset**: Sentences with token-level entity tags (e.g., \"B-geo\" for beginning of a geographic entity, \"O\" for no entity).\n",
    "https://www.kaggle.com/datasets/rajnathpatel/ner-data\n",
    "\n",
    "\n",
    "## 1.4 Data Processing\n",
    "\n",
    "### 1.4.1 Cleaning the Data\n",
    "\n",
    "- **Classification Data**:\n",
    "  - Dropped rows with missing `review` or `sentiment`.\n",
    "  - Cleaned text by stripping HTML tags, special characters, and extra whitespace.\n",
    "  - Filtered to ensure only \"positive\" or \"negative\" labels.\n",
    "\n",
    "- **NER Data**:\n",
    "  - Filled missing `Sentence #` values to group words into sentences.\n",
    "  - Removed rows with missing `Word` or `Tag`.\n",
    "  - Excluded sentences with invalid tags for consistency.\n",
    "\n",
    "### 1.4.2 Wrapping Up with Dataset and DataLoader\n",
    "\n",
    "I created a custom `MultiTaskDataset` class to handle both tasks:\n",
    "- **Classification**: Tokenizes reviews and maps labels to 0 (negative) or 1 (positive).\n",
    "- **NER**: Tokenizes word lists, aligns labels with subword tokens using `word_ids`, and assigns `-100` to ignored tokens (padding/subwords).\n",
    "\n",
    "A custom `collate_fn` batches the data properly for the `DataLoader`, ensuring everything lines up for training.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 Evaluation Metrics and Demo\n",
    "\n",
    "To see how well the model performs, I implemented task-specific metrics and a prediction demo:\n",
    "\n",
    "### 1.5.1 Classification Metrics\n",
    "- **Accuracy**: Percentage of correctly predicted sentiments.\n",
    "- **Classification Report**: Precision, recall, and F1-score for \"negative\" and \"positive\" classes.\n",
    "\n",
    "### 1.5.2 NER Metrics\n",
    "- **Entity-Level Metrics**: Precision, recall, and F1-score using `seqeval`, which evaluates NER at the entity level and ignores `-100` labels.\n",
    "\n",
    "### 1.5.3 Prediction Demo\n",
    "I’ll show how to:\n",
    "- Run the model on sample inputs for both tasks.\n",
    "- Compare predicted labels to true labels.\n",
    "- Calculate and display the metrics above.\n",
    "\n",
    "This demo will let you see the model in action and verify its performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Summary of Key Decisions\n",
    "\n",
    "### 1.6.1 Model Design\n",
    "- **Shared BERT Backbone**: `bert-base-uncased` for efficiency and pre-trained knowledge.\n",
    "- **Task Heads**: Separate linear layers for classification and NER.\n",
    "- **Pooling**: Optional `[CLS]` or mean pooling for classification.\n",
    "\n",
    "### 1.6.2 Data Handling\n",
    "- **Cleaning**: Removed invalid entries and normalized text.\n",
    "- **Tokenization**: Used BERT’s tokenizer with subword alignment for NER.\n",
    "\n",
    "### 1.6.3 Evaluation\n",
    "- **Metrics**: Accuracy and F1 for classification, entity-level F1 for NER.\n",
    "- **Demo**: Predicts embeddings and calculates metrics for both tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387077e",
   "metadata": {},
   "source": [
    "# 2 Implementation MultiTaskSentenceTransformer of  Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d25cf",
   "metadata": {},
   "source": [
    "## 2.1 Imports\n",
    "The code begins with necessary imports for data handling, model building, and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fd8324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a3ddf",
   "metadata": {},
   "source": [
    "## 2.2 Data Loading and Preparation for NLP Tasks\n",
    "\n",
    "- Loads and preprocesses XLSX datasets for **classification** (sentiment) and **NER** (entity tagging).\n",
    "- Cleans text, caches processed data, and prepares PyTorch `DataLoader`s for training/evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ab76a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_label_map = {\n",
    "    \"O\": 0,\n",
    "    \"B-art\": 1, \"I-art\": 2,  # Artifact\n",
    "    \"B-eve\": 3, \"I-eve\": 4,  # Event\n",
    "    \"B-geo\": 5, \"I-geo\": 6,  # Geographical Entity\n",
    "    \"B-gpe\": 7, \"I-gpe\": 8,  # Countries, Cities, States\n",
    "    \"B-nat\": 9, \"I-nat\": 10,  # Natural Phenomena\n",
    "    \"B-org\": 11, \"I-org\": 12,  # Organizations\n",
    "    \"B-per\": 13, \"I-per\": 14,  # Persons\n",
    "    \"B-tim\": 15, \"I-tim\": 16,  # Time Expressions\n",
    "}\n",
    "\n",
    "# Directory for storing processed datasets\n",
    "PROCESSED_DIR = \"./data/processed/\"\n",
    "\n",
    "\n",
    "def load_data_from_xlsx(file_path, task):\n",
    "    \"\"\"\n",
    "    Load and preprocess dataset from an XLSX file.\n",
    "\n",
    "    - Classification task expects 'review' and 'sentiment' columns.\n",
    "    - NER task expects 'Sentence #', 'Word', and 'Tag' columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        required_columns = [\"review\", \"sentiment\"]\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(\"Classification XLSX must have 'review' and 'sentiment' columns.\")\n",
    "\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df['review'] = df['review'].astype(str).apply(clean_text)\n",
    "\n",
    "        # Filter only valid sentiment labels\n",
    "        valid_sentiments = {\"positive\", \"negative\"}\n",
    "        df = df[df['sentiment'].isin(valid_sentiments)]\n",
    "\n",
    "        data = [{\"sentence\": row[\"review\"], \"label\": row[\"sentiment\"]} for _, row in df.iterrows()]\n",
    "\n",
    "    elif task == \"ner\":\n",
    "        required_columns = [\"Sentence #\", \"Word\", \"Tag\"]\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(\"NER XLSX must have 'Sentence #', 'Word', and 'Tag' columns.\")\n",
    "\n",
    "        df['Sentence #'] = df['Sentence #'].ffill()  # Fill missing sentence numbers\n",
    "        df = df.dropna(subset=[\"Word\", \"Tag\"])\n",
    "\n",
    "        # Define valid NER tags\n",
    "        valid_tags = {\n",
    "            \"B-art\", \"B-eve\", \"B-geo\", \"B-gpe\", \"B-nat\", \"B-org\", \"B-per\", \"B-tim\",\n",
    "            \"I-art\", \"I-eve\", \"I-geo\", \"I-gpe\", \"I-nat\", \"I-org\", \"I-per\", \"I-tim\", \"O\"\n",
    "        }\n",
    "\n",
    "        # Group words and tags by sentence\n",
    "        grouped = df.groupby('Sentence #')\n",
    "        data = []\n",
    "        for _, group in grouped:\n",
    "            words = group['Word'].astype(str).tolist()\n",
    "            tags = group['Tag'].astype(str).tolist()\n",
    "            if all(tag in valid_tags for tag in tags):\n",
    "                data.append({'words': words, 'labels': tags})\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task}. Use 'classification' or 'ner'.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by removing HTML tags and special characters.\"\"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?'-]\", \"\", text)  # Remove unwanted characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_processed_data(data, filename):\n",
    "    \"\"\"Save processed dataset using pickle.\"\"\"\n",
    "    filepath = os.path.join(PROCESSED_DIR, filename)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Processed data saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_processed_data(filename):\n",
    "    \"\"\"Load processed dataset from cache if available.\"\"\"\n",
    "    filepath = os.path.join(PROCESSED_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Loaded cached data: {filepath}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "\n",
    "def prepare_classification_data(file_path, batch_size=8, test_size=100, seed=42):\n",
    "    \"\"\"Loads and prepares classification dataset with caching.\"\"\"\n",
    "    cache_filename = \"classification_dataset.pkl\"\n",
    "\n",
    "    # Load cached data if available\n",
    "    cached_data = load_processed_data(cache_filename)\n",
    "    if cached_data:\n",
    "        classification_train_data, classification_test_data = cached_data\n",
    "    else:\n",
    "        # Load raw data from file\n",
    "        classification_data = load_data_from_xlsx(file_path, \"classification\")\n",
    "        train_size = len(classification_data) - test_size\n",
    "\n",
    "        # Split dataset into training and test sets\n",
    "        classification_train_data, classification_test_data = random_split(\n",
    "            classification_data, [train_size, test_size], generator=torch.Generator().manual_seed(seed)\n",
    "        )\n",
    "\n",
    "        # Cache processed data\n",
    "        save_processed_data((classification_train_data, classification_test_data), cache_filename)\n",
    "\n",
    "    # Create datasets\n",
    "    classification_train_dataset = MultiTaskDataset(task=\"classification\", data=classification_train_data, tokenizer=tokenizer)\n",
    "    classification_test_dataset = MultiTaskDataset(task=\"classification\", data=classification_test_data, tokenizer=tokenizer)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    classification_train_loader = DataLoader(classification_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    classification_test_loader = DataLoader(classification_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    return classification_train_loader, classification_test_loader, len(classification_train_dataset), len(classification_test_dataset)\n",
    "\n",
    "\n",
    "def prepare_ner_data(file_path, batch_size=8, test_size=100, seed=42, ner_label_map=ner_label_map):\n",
    "    \"\"\"Loads and prepares NER dataset with caching.\"\"\"\n",
    "    cache_filename = \"ner_dataset.pkl\"\n",
    "\n",
    "    # Load cached data if available\n",
    "    cached_data = load_processed_data(cache_filename)\n",
    "    if cached_data:\n",
    "        ner_train_data, ner_test_data = cached_data\n",
    "    else:\n",
    "        # Load raw data from file\n",
    "        ner_data = load_data_from_xlsx(file_path, \"ner\")\n",
    "        train_size = len(ner_data) - test_size\n",
    "\n",
    "        # Split dataset into training and test sets\n",
    "        ner_train_data, ner_test_data = random_split(\n",
    "            ner_data, [train_size, test_size], generator=torch.Generator().manual_seed(seed)\n",
    "        )\n",
    "\n",
    "        # Cache processed data\n",
    "        save_processed_data((ner_train_data, ner_test_data), cache_filename)\n",
    "\n",
    "    # Create datasets\n",
    "    ner_train_dataset = MultiTaskDataset(task=\"ner\", data=ner_train_data, tokenizer=tokenizer, label_map=ner_label_map)\n",
    "    ner_test_dataset = MultiTaskDataset(task=\"ner\", data=ner_test_data, tokenizer=tokenizer, label_map=ner_label_map)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    ner_train_loader = DataLoader(ner_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    ner_test_loader = DataLoader(ner_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    return ner_train_loader, ner_test_loader, len(ner_train_dataset), len(ner_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203deff5",
   "metadata": {},
   "source": [
    "## 2.3 Dataset: `MultiTaskDataset`\n",
    "- **Initialize** a custom PyTorch dataset for classification or NER tasks using task-specific data and a tokenizer.\n",
    "- **Retrieve and tokenize** individual samples, mapping labels to integers for classification or aligning NER labels with sub-tokens.\n",
    "- **Return** formatted inputs and labels as tensors, ensuring compatibility with PyTorch's `DataLoader` for efficient batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a0fb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "class MultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for multi-task learning, supporting classification and NER tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, task, data, tokenizer, label_map=None, max_length=512):\n",
    "        self.task = task\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Tokenizes input and returns processed tensors for the specified task.\"\"\"\n",
    "        if self.task == \"classification\":\n",
    "            text = self.data[idx][\"sentence\"]\n",
    "            label = 0 if self.data[idx][\"label\"] == \"negative\" else 1\n",
    "            inputs = self.tokenizer(text, truncation=True, padding=\"max_length\",\n",
    "                                    max_length=self.max_length, return_tensors=\"pt\")\n",
    "            return {k: v.squeeze(0) for k, v in inputs.items()}, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        elif self.task == \"ner\":\n",
    "            words = self.data[idx][\"words\"]\n",
    "            labels = self.data[idx][\"labels\"]\n",
    "            inputs = self.tokenizer(words, is_split_into_words=True, truncation=True,\n",
    "                                    padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "            word_ids = inputs.word_ids(batch_index=0) if hasattr(inputs, \"word_ids\") else None\n",
    "\n",
    "            # Initialize all label positions as ignored tokens\n",
    "            label_ids = [-100] * len(inputs[\"input_ids\"][0])\n",
    "\n",
    "            if word_ids:\n",
    "                for i, word_id in enumerate(word_ids):\n",
    "                    if word_id is not None and word_id < len(labels):\n",
    "                        if i == 0 or word_ids[i - 1] != word_id:\n",
    "                            label_ids[i] = self.label_map.get(labels[word_id], -100) # Assign B-label\n",
    "                        else:\n",
    "                            label_ids[i] = self.label_map.get(f\"I-{labels[word_id][2:]}\", -100)  # Assign I-label\n",
    "\n",
    "            return {k: v.squeeze(0) for k, v in inputs.items()}, torch.tensor(label_ids, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task: {self.task}. Use 'classification' or 'ner'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8160d68",
   "metadata": {},
   "source": [
    "## 2.4 DataLoader: `custom_collate_fn`\n",
    "- **Purpose**: Batches data for the `DataLoader` by combining individual samples into properly structured tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77fa244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom function to batch tokenized inputs and labels.\n",
    "    Ensures proper tensor stacking.\n",
    "    \"\"\"\n",
    "    inputs = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    batched_inputs = {key: torch.stack([item[key] for item in inputs]) for key in inputs[0]}\n",
    "    batched_labels = torch.stack(labels)\n",
    "\n",
    "    return batched_inputs, batched_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f0991",
   "metadata": {},
   "source": [
    "## 2.5 Data Setup and Example Usage\n",
    "The code sets up datasets and dataloaders for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76f9aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_data_loading(classification_file, ner_file, sample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Demonstrates loading of classification and NER datasets with optional sampling.\n",
    "    Allows loading only a fraction of the dataset for quick testing.\n",
    "\n",
    "    Args:\n",
    "        classification_file (str): Path to classification dataset file.\n",
    "        ner_file (str): Path to NER dataset file.\n",
    "        sample_ratio (float): Fraction of data to sample (0.0 to 1.0).\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Train/test DataLoaders for classification and NER tasks.\n",
    "    \"\"\"\n",
    "    assert 0.0 < sample_ratio <= 1.0, \"sample_ratio must be between 0 and 1\"\n",
    "\n",
    "    # Load full datasets\n",
    "    classification_train_loader, classification_test_loader, classification_train_size, classification_test_size = prepare_classification_data(\n",
    "        classification_file)\n",
    "    ner_train_loader, ner_test_loader, ner_train_size, ner_test_size = prepare_ner_data(ner_file)\n",
    "\n",
    "    def sample_dataloader(dataloader, sample_ratio):\n",
    "        \"\"\"Randomly samples a fraction of a DataLoader's dataset.\"\"\"\n",
    "        dataset = dataloader.dataset\n",
    "        sampled_size = max(1, int(len(dataset) * sample_ratio))  # Ensure at least one sample\n",
    "        sampled_indices = random.sample(range(len(dataset)), sampled_size)\n",
    "        sampled_dataset = torch.utils.data.Subset(dataset, sampled_indices)\n",
    "        return torch.utils.data.DataLoader(sampled_dataset, batch_size=dataloader.batch_size, shuffle=True,\n",
    "                                           collate_fn=dataloader.collate_fn)\n",
    "\n",
    "    # Apply sampling\n",
    "    if sample_ratio < 1.0:\n",
    "        classification_train_loader = sample_dataloader(classification_train_loader, sample_ratio)\n",
    "        classification_test_loader = sample_dataloader(classification_test_loader, sample_ratio)\n",
    "        ner_train_loader = sample_dataloader(ner_train_loader, sample_ratio)\n",
    "        ner_test_loader = sample_dataloader(ner_test_loader, sample_ratio)\n",
    "\n",
    "    # Print dataset sizes\n",
    "    print(\n",
    "        f\"Classification Train Size: {len(classification_train_loader.dataset)}, Test Size: {len(classification_test_loader.dataset)}\")\n",
    "    print(f\"NER Train Size: {len(ner_train_loader.dataset)}, Test Size: {len(ner_test_loader.dataset)}\")\n",
    "\n",
    "    # Display a sample batch for classification\n",
    "    for batch_inputs, batch_labels in classification_train_loader:\n",
    "        print(\"\\n Classification Batch Sample:\")\n",
    "        print(\"Input IDs shape:\", batch_inputs[\"input_ids\"].shape)\n",
    "        print(\"Labels shape:\", batch_labels.shape)\n",
    "        break  # Show only the first batch\n",
    "\n",
    "    # Display a sample batch for NER\n",
    "    for batch_inputs, batch_labels in ner_train_loader:\n",
    "        print(\"\\n NER Batch Sample:\")\n",
    "        print(\"Input IDs shape:\", batch_inputs[\"input_ids\"].shape)\n",
    "        print(\"Labels shape:\", batch_labels.shape)\n",
    "        break  # Show only the first batch\n",
    "\n",
    "    return classification_train_loader, classification_test_loader, ner_train_loader, ner_test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b46bc",
   "metadata": {},
   "source": [
    "## 2.6 Model: `MultiTaskSentenceTransformer`\n",
    "- **Purpose**: Extends `SentenceTransformer` to perform sentence classification and NER simultaneously using a shared transformer backbone.\n",
    "- **Structure**: Adds two linear heads—a classification head for sentence-level predictions and an NER head for token-level predictions, with configurable pooling (`\"cls\"` or `\"mean\"`).\n",
    "- **Forward Pass**: Processes inputs through the transformer, applies pooling for classification logits, and generates per-token NER logits, returning both outputs for multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb4dc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskSentenceTransformer(SentenceTransformer):\n",
    "    \"\"\"\n",
    "    Multi-task Sentence Transformer supporting both classification and NER tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_length,\n",
    "                 num_classes=2, num_entity_labels=17, pooling=\"cls\", layer_norm_eps=1e-12):\n",
    "        super().__init__(vocab_size=vocab_size, embed_size=embed_size, num_heads=num_heads, hidden_dim=hidden_dim,\n",
    "                         num_layers=num_layers, max_length=max_length, pooling=pooling, output_mode=\"token\",\n",
    "                         layer_norm_eps=layer_norm_eps)\n",
    "\n",
    "        # Output layers for classification and named entity recognition (NER)\n",
    "        self.classification_head = nn.Linear(embed_size, num_classes)\n",
    "        self.ner_head = nn.Linear(embed_size, num_entity_labels)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        Processes input through the transformer and generates outputs for classification and NER.\n",
    "        \"\"\"\n",
    "        sequence_output = super().forward(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        # Apply pooling strategy to get sentence-level representation\n",
    "        if self.pooling == \"cls\":\n",
    "            cls_output = sequence_output[:, 0, :]  # Use CLS token output\n",
    "        elif self.pooling == \"mean\":\n",
    "            if attention_mask is not None:\n",
    "                masked_output = sequence_output * attention_mask.unsqueeze(-1)\n",
    "                cls_output = masked_output.sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(\n",
    "                    -1)  # Mean pooling with masking\n",
    "            else:\n",
    "                cls_output = sequence_output.mean(dim=1)  # Simple mean pooling\n",
    "\n",
    "        # Generate logits for classification and NER\n",
    "        classification_logits = self.classification_head(cls_output)\n",
    "        ner_logits = self.ner_head(sequence_output)\n",
    "\n",
    "        return classification_logits, ner_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d358e",
   "metadata": {},
   "source": [
    "## 2.7 Evaluation Functions\n",
    "- **Classification Evaluation**: Assesses model performance on sentence classification by collecting predictions and true labels, computing accuracy, and generating a detailed report (precision, recall, F1) for \"negative\" and \"positive\" classes.\n",
    "- **NER Evaluation**: Evaluates NER performance by gathering per-token predictions and labels, filtering out `-100` (padding) tokens, mapping to string labels via `id_to_label`, and producing an entity-level report (precision, recall, F1) using `seqeval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c93e1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(model, dataloader, device=torch.device('cpu')):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}  # Move inputs to device\n",
    "            batch_labels = batch_labels.to(device)  # Move labels to device\n",
    "\n",
    "            classification_logits, _ = model(**batch_inputs)\n",
    "            preds = torch.argmax(classification_logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    # Ensure at least two classes are present\n",
    "    unique_classes = set(all_preds) | set(all_labels)\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Warning: Only one class detected in predictions: {unique_classes}. Adjusting labels.\")\n",
    "        target_names = [str(cls) for cls in unique_classes]  # Dynamically assign class names\n",
    "    else:\n",
    "        target_names = [\"negative\", \"positive\"]\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "\n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "# Evaluate Named Entity Recognition (NER) performance\n",
    "def evaluate_ner(model, dataloader, id_to_label, device=torch.device(\"cpu\")):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            _, ner_logits = model(**batch_inputs)\n",
    "            preds = torch.argmax(ner_logits, dim=-1).cpu().numpy()\n",
    "            labels = batch_labels.cpu().numpy()\n",
    "\n",
    "            for i in range(preds.shape[0]):\n",
    "                seq_preds = []\n",
    "                seq_labels = []\n",
    "                for j in range(preds.shape[1]):\n",
    "                    if labels[i, j] != -100:  # Ignore padding tokens\n",
    "                        seq_preds.append(id_to_label.get(preds[i, j], \"O\"))  # Ensure default 'O'\n",
    "                        seq_labels.append(id_to_label.get(labels[i, j], \"O\"))\n",
    "\n",
    "                all_preds.append(seq_preds)\n",
    "                all_labels.append(seq_labels)\n",
    "\n",
    "    # **DEBUGGING OUTPUT**\n",
    "    print(\"\\nNER Evaluation Debugging:\")\n",
    "    print(f\"Total Sentences Evaluated: {len(all_labels)}\")\n",
    "    if all_labels:\n",
    "        print(f\"First Prediction Example: {all_preds[0]}\")\n",
    "        print(f\"First Label Example: {all_labels[0]}\")\n",
    "\n",
    "    if not any(any(label != \"O\" for label in labels) for labels in all_labels):\n",
    "        print(\"Warning: No Named Entities Found in Labels! Possible Dataset Issue.\")\n",
    "\n",
    "    if not any(any(label != \"O\" for label in labels) for labels in all_preds):\n",
    "        print(\"Warning: Model is Predicting Only 'O' Labels! Check Training.\")\n",
    "\n",
    "    # **Ensure we don't pass an empty list to seqeval**\n",
    "    if not all_labels or not all_preds:\n",
    "        raise ValueError(\"Empty sequences found! Check dataset and model output.\")\n",
    "\n",
    "    return seqeval_report(all_labels, all_preds, mode='strict', scheme=IOB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db18e6b",
   "metadata": {},
   "source": [
    "## 2.9 Demonstration and Evaluation\n",
    "The code demonstrates predictions and evaluations for both tasks.\n",
    "Because the model is untrained, with randomly initialized classification and NER heads, leading to poor performance:\n",
    "- **Random Predictions**: Untrained heads produce near-random outputs, resulting in low accuracy and poor F1 scores for classification and NER.\n",
    "- **Training Needed**: Performance will improve after training the heads on the datasets using appropriate loss functions and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8f410ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30522\n",
      "Embedding Size: 768\n",
      "Number of Heads: 12\n",
      "Hidden Dimension: 3072\n",
      "Number of Layers: 12\n",
      "Max Position Embeddings: 512\n",
      "Layer Norm Epsilon: 1e-12\n",
      "Weights successfully copied from pretrained BERT to custom model!\n",
      "\n",
      "Custom model initialized and weights copied successfully.\n",
      "Loaded cached data: ./data/processed/classification_dataset.pkl\n",
      "Loaded cached data: ./data/processed/ner_dataset.pkl\n",
      "Classification Train Size: 4989, Test Size: 10\n",
      "NER Train Size: 4775, Test Size: 10\n",
      "\n",
      " Classification Batch Sample:\n",
      "Input IDs shape: torch.Size([8, 512])\n",
      "Labels shape: torch.Size([8])\n",
      "\n",
      " NER Batch Sample:\n",
      "Input IDs shape: torch.Size([8, 512])\n",
      "Labels shape: torch.Size([8, 512])\n",
      "Predicted classes: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "True labels: [1, 0, 1, 1, 0, 1, 1, 1]\n",
      "Predicted NER labels: [[8, 2, 5, 5, 15, 5, 1, 5, 1, 5, 5, 16, 15, 15, 5, 2, 2, 5, 5, 5, 15, 2, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 15, 15, 4, 11, 4, 4, 16, 1, 4, 1, 15, 5, 5, 5, 5, 15, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 11, 4, 11, 4, 16, 4, 1, 15, 2, 2, 5, 5, 15, 5, 4, 5, 4, 15, 5, 5, 5, 5, 5, 5, 4, 4, 4, 15, 4, 1, 11, 4, 4, 16, 6, 4, 1, 2, 5, 5, 4, 15, 15, 5, 5, 4, 15, 5, 5, 5, 5, 2, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 15, 5, 5, 5, 4, 15, 5, 5, 4, 5, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 15, 4, 11, 11, 15, 11, 16, 16, 1, 1, 2, 2, 5, 15, 5, 2, 15, 5, 5, 15, 2, 5, 5, 5, 15, 5, 5, 4, 4, 4, 4, 5, 4, 4, 15, 4, 11, 11, 4, 15, 16, 6, 2, 2, 1, 2, 2, 2, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 1, 15, 4, 4, 6, 1, 15, 2, 5, 5, 15, 15, 5, 5, 5, 4, 15, 4, 15, 5, 5, 15, 2, 2, 2, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 4, 6, 15, 15, 15, 2, 5, 2, 5, 15, 15, 5, 5, 5, 15, 15, 5, 5, 5, 15, 15, 15, 5, 5, 5, 15, 15, 5, 5, 5, 4, 5, 4, 4, 5, 4, 4, 15, 15, 4, 6, 4, 6, 15, 2, 15, 0, 5, 15, 5, 5, 15, 4, 15, 15, 4, 4, 4, 6, 4, 6, 2, 2, 2, 15, 15, 5, 15, 15, 5, 5, 15, 15, 15, 15, 15, 15, 15, 15, 5, 5, 5, 4, 15, 15, 4, 5, 5, 4, 15, 4, 5, 5, 15, 2, 5, 15, 2, 2, 15, 15, 15, 5, 2, 15, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 15, 5, 15, 15, 5, 5, 5, 5, 15, 5, 5, 4, 4, 5, 4, 5, 4, 4, 4, 4, 1, 15, 1, 15, 5, 4, 2, 1, 1, 2, 2, 2, 2, 5, 15, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 16, 4, 16, 4, 6, 6, 1, 15, 15, 5, 4, 1, 2, 0, 4, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 2, 5, 2, 5, 5, 4, 4, 4, 4, 5, 4, 4, 1, 4, 16, 16, 6, 4, 1, 15, 2, 5, 15, 15, 5, 2, 5, 5, 5, 5, 15, 15, 5, 5, 5, 15, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 6, 2, 15, 5, 5, 2, 15, 5, 5, 5], [14, 7, 6, 9, 5, 9, 1, 11, 11, 3, 11, 10, 5, 3, 15, 14, 11, 14, 6, 7, 14, 12, 1, 2, 5, 14, 1, 12, 8, 7, 7, 7, 7, 9, 7, 11, 6, 6, 11, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 0, 7, 7, 7, 7, 7, 15, 9, 9, 9, 9, 11, 11, 6, 6, 11, 11, 6, 11, 10, 11, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 15, 7, 7, 7, 7, 7, 7, 9, 9, 11, 11, 6, 6, 7, 15, 6, 7, 11, 14, 7, 7, 7, 7, 6, 9, 7, 7, 7, 15, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 15, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 11, 11, 11, 6, 11, 6, 11, 7, 11, 3, 11, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 11, 11, 6, 11, 7, 7, 11, 6, 6, 6, 7, 2, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 15, 9, 7, 7, 7, 7, 7, 9, 6, 7, 15, 7, 11, 10, 7, 7, 9, 7, 7, 7, 5, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 9, 7, 7, 7, 15, 7, 9, 9, 7, 7, 7, 9, 7, 7, 9, 7, 7, 15, 7, 7, 2, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 6, 9, 7, 7, 7, 7, 9, 9, 7, 7, 9, 7, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 8, 8, 9, 7, 2, 7, 7, 7, 7, 7, 7, 7, 9, 9, 7, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 9, 9, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 2, 7, 7, 9, 7, 2, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 11, 11, 6, 11, 9, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 7, 9, 7, 7, 7, 15, 7, 7, 7, 7, 7, 9, 7, 9, 11, 11, 6, 6, 11, 6, 6, 7, 1, 10, 7, 7, 7, 7, 14, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 2, 7, 7, 2, 7, 7, 9, 7, 9, 7, 7, 7, 9, 7, 11, 0, 11, 6, 6, 6, 11, 10, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 15, 9, 7, 7, 7, 7, 7, 9, 6, 7, 7, 6, 7, 2, 7, 10, 10, 7, 7, 7, 7, 7, 7, 7], [7, 10, 5, 11, 5, 5, 5, 5, 5, 5, 12, 5, 7, 7, 5, 9, 11, 11, 5, 5, 5, 2, 11, 5, 15, 7, 15, 11, 9, 5, 7, 5, 14, 5, 5, 5, 5, 2, 11, 5, 2, 11, 11, 5, 5, 5, 5, 5, 5, 5, 5, 2, 11, 6, 6, 7, 15, 9, 11, 5, 5, 5, 6, 7, 7, 2, 9, 1, 11, 5, 2, 11, 5, 5, 5, 2, 1, 5, 1, 5, 2, 2, 11, 5, 7, 6, 7, 11, 5, 5, 6, 6, 4, 7, 5, 2, 11, 5, 1, 1, 2, 1, 5, 5, 5, 5, 9, 1, 5, 2, 11, 15, 6, 6, 5, 5, 5, 5, 5, 1, 2, 1, 1, 9, 5, 2, 0, 0, 5, 5, 1, 7, 11, 5, 5, 6, 0, 4, 4, 15, 2, 11, 7, 15, 11, 6, 5, 11, 1, 5, 5, 7, 6, 6, 7, 7, 2, 1, 7, 5, 7, 1, 11, 7, 1, 7, 7, 2, 7, 7, 2, 7, 6, 6, 7, 7, 15, 9, 11, 5, 14, 7, 6, 7, 2, 2, 1, 15, 7, 2, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 6, 5, 15, 2, 15, 1, 11, 5, 15, 7, 6, 7, 7, 5, 5, 6, 4, 7, 2, 9, 1, 5, 2, 1, 1, 5, 5, 5, 5, 1, 5, 5, 2, 1, 1, 1, 5, 2, 1, 15, 1, 1, 5, 5, 5, 5, 5, 1, 5, 5, 5, 6, 1, 1, 5, 5, 5, 1, 5, 5, 2, 7, 2, 1, 5, 2, 1, 1, 1, 7, 5, 5, 7, 1, 7, 7, 7, 7, 2, 7, 7, 7, 2, 1, 7, 1, 7, 7, 1, 1, 7, 7, 7, 5, 1, 6, 6, 1, 7, 2, 2, 1, 7, 7, 1, 7, 7, 1, 1, 0, 7, 7, 5, 7, 7, 11, 1, 7, 7, 2, 7, 7, 7, 7, 7, 2, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 1, 1, 5, 2, 7, 5, 1, 4, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 5, 6, 6, 11, 5, 5, 7, 7, 6, 7, 2, 1, 5, 2, 1, 1, 1, 11, 5, 5, 1, 11, 5, 5, 5, 5, 1, 1, 5, 5, 6, 4, 7, 2, 7, 7, 1, 7, 7, 7, 1, 7, 7, 7, 7, 7, 1, 11, 5, 15, 11, 5, 5, 5, 5, 5, 5, 5, 2, 5, 7, 7, 7, 2, 7, 1, 5, 5, 2, 7, 7, 7, 7, 2, 2, 0, 7, 0, 0, 0, 7, 7, 7, 2, 0, 1, 1, 7, 6, 1, 11, 5, 5, 5, 5, 5, 5, 6, 4, 7, 2, 1, 7, 7, 7, 7, 1, 7, 7, 1, 7, 7, 2, 7, 7, 7, 7, 1, 7, 7, 7, 7, 1, 1, 1, 5, 5, 7, 2, 1, 7, 1, 7, 7, 2, 7, 7, 7, 7, 2, 7, 7, 7], [14, 7, 8, 8, 4, 8, 8, 5, 5, 5, 14, 14, 14, 10, 10, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 6, 6, 6, 8, 11, 7, 7, 7, 7, 11, 7, 11, 11, 1, 2, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 15, 0, 0, 4, 6, 9, 11, 11, 7, 7, 7, 7, 11, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 9, 11, 1, 1, 7, 7, 7, 11, 11, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 9, 11, 11, 0, 0, 7, 7, 7, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 15, 0, 0, 7, 7, 7, 0, 7, 15, 0, 0, 9, 0, 6, 9, 11, 1, 1, 7, 7, 7, 11, 11, 1, 7, 7, 0, 11, 1, 11, 11, 0, 0, 2, 0, 0, 0, 0, 0, 15, 6, 0, 15, 0, 9, 11, 6, 7, 7, 7, 1, 1, 7, 7, 7, 11, 7, 2, 1, 2, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 11, 11, 1, 7, 7, 7, 11, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 0, 1, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 11, 11, 7, 7, 7, 7, 11, 7, 7, 2, 1, 11, 1, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 9, 11, 1, 0, 7, 7, 7, 7, 7, 11, 1, 7, 7, 11, 11, 0, 0, 0, 0, 0, 11, 0, 0, 6, 1, 7, 7, 7, 1, 7, 1, 11, 11, 0, 0, 11, 0, 0, 0, 15, 15, 11, 7, 0, 0, 0, 11, 0, 0, 0, 0, 0, 7, 11, 11, 11, 11, 7, 7, 1, 11, 0, 11, 0, 0, 7, 11, 0, 0, 0, 0, 11, 0, 0, 3, 0, 0, 0, 11, 0, 1, 0, 0, 7, 0, 0, 7, 0, 0, 0, 1, 11, 0, 0, 6, 6, 6, 11, 0, 0, 6, 9, 11, 1, 7, 7, 7, 7, 7, 7, 7, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 7, 1, 1, 7, 7, 7, 11, 11, 1, 7, 0, 1, 11, 1, 7, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 3, 0, 4, 0, 9, 11, 0, 9, 11, 1, 7, 12, 7, 7, 11, 1, 7, 11, 0, 0, 15, 0, 11, 0, 0, 0, 15, 11, 7, 7, 1, 0, 11, 0, 0, 0, 0, 11, 11, 7, 7, 7, 7, 7, 1, 7, 11, 11, 7, 7, 11, 0, 0, 0], [7, 15, 5, 5, 5, 5, 6, 15, 15, 2, 5, 15, 8, 15, 6, 5, 12, 8, 9, 9, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 9, 9, 4, 4, 6, 4, 3, 6, 3, 6, 4, 7, 4, 9, 4, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 9, 9, 4, 4, 6, 3, 3, 6, 6, 15, 4, 4, 9, 9, 9, 9, 7, 9, 7, 7, 0, 7, 0, 5, 7, 7, 7, 7, 7, 6, 7, 4, 4, 6, 3, 3, 6, 6, 7, 15, 4, 4, 9, 9, 9, 9, 7, 0, 0, 7, 7, 1, 7, 7, 7, 7, 1, 6, 7, 7, 6, 7, 7, 7, 6, 6, 4, 9, 7, 7, 7, 9, 9, 7, 7, 7, 7, 7, 7, 7, 3, 7, 7, 6, 7, 7, 7, 7, 9, 7, 7, 7, 4, 4, 6, 3, 3, 6, 6, 4, 15, 4, 4, 9, 9, 0, 0, 9, 9, 0, 7, 7, 9, 9, 7, 7, 7, 7, 6, 6, 9, 9, 4, 4, 3, 6, 6, 4, 6, 3, 3, 6, 6, 7, 4, 9, 4, 9, 9, 3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 4, 4, 6, 3, 6, 6, 15, 4, 4, 15, 7, 7, 9, 9, 7, 7, 7, 7, 0, 9, 7, 7, 7, 4, 9, 3, 0, 1, 7, 7, 9, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 6, 6, 7, 4, 4, 7, 6, 4, 7, 7, 9, 7, 9, 7, 9, 1, 0, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 9, 7, 9, 9, 9, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 4, 7, 9, 9, 0, 0, 7, 7, 7, 7, 7, 7, 0, 7, 6, 6, 6, 4, 7, 4, 9, 7, 9, 9, 7, 7, 9, 7, 7, 7, 0, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 6, 7, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 9, 0, 7, 0, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 6, 7, 9, 3, 7, 7, 7, 7, 7, 5, 7, 7, 4, 6, 3, 6, 7, 7, 6, 7, 7, 7, 7, 9, 0, 9, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 6, 6, 3, 6, 6, 4, 15, 4, 4, 9, 7, 11, 7, 7, 9, 0, 7, 0, 0, 0, 12, 7, 0, 7, 7, 7, 7, 1, 0, 7, 6, 6, 7, 6, 7, 7, 6, 4, 6, 3, 6, 6, 4, 15, 4, 7, 9, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 7, 7, 3, 7, 1, 4, 6, 9, 4, 4, 7, 7, 7, 9, 7, 0, 7, 7], [11, 8, 8, 5, 12, 9, 11, 5, 9, 11, 9, 11, 7, 5, 5, 5, 15, 9, 2, 2, 3, 9, 9, 10, 5, 9, 8, 8, 12, 8, 9, 9, 9, 9, 9, 9, 9, 11, 11, 9, 11, 11, 6, 6, 9, 9, 9, 7, 9, 9, 7, 9, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 6, 9, 6, 11, 11, 11, 6, 9, 3, 9, 9, 9, 7, 9, 9, 9, 11, 11, 9, 9, 2, 11, 9, 9, 9, 9, 9, 9, 6, 6, 9, 9, 11, 11, 2, 6, 11, 9, 15, 7, 9, 9, 7, 9, 9, 9, 9, 9, 2, 9, 2, 9, 9, 9, 9, 11, 9, 9, 11, 9, 11, 9, 9, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 6, 9, 9, 11, 11, 11, 6, 9, 8, 15, 7, 9, 9, 7, 9, 9, 9, 9, 11, 9, 2, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 9, 11, 9, 9, 11, 11, 9, 9, 11, 2, 2, 9, 7, 7, 9, 7, 9, 9, 9, 11, 11, 11, 9, 2, 9, 7, 9, 9, 9, 9, 9, 11, 11, 9, 9, 11, 2, 11, 9, 9, 2, 2, 9, 9, 7, 7, 7, 7, 9, 7, 9, 9, 15, 9, 9, 9, 2, 7, 9, 7, 9, 7, 9, 11, 9, 11, 9, 9, 9, 2, 9, 9, 9, 7, 9, 9, 7, 9, 7, 9, 9, 2, 11, 9, 9, 9, 9, 9, 7, 9, 7, 9, 9, 7, 9, 9, 9, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 15, 11, 9, 7, 9, 9, 7, 9, 11, 9, 7, 7, 11, 11, 11, 11, 11, 9, 7, 9, 9, 9, 9, 9, 9, 9, 7, 7, 11, 9, 9, 11, 11, 7, 9, 9, 9, 9, 7, 9, 11, 11, 9, 9, 11, 11, 11, 9, 11, 9, 9, 7, 7, 7, 7, 9, 7, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 11, 9, 9, 9, 9, 9, 9, 9, 7, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 7, 9, 11, 11, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 7, 11, 9, 11, 11, 11, 15, 11, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 7, 9, 10, 11, 11, 11, 9, 9, 9, 7, 9, 9, 9, 11, 9, 9, 9, 9, 9, 9, 11, 9, 9, 11, 9, 11, 11, 9, 11, 9, 9, 9, 9, 7, 9, 9, 9, 11, 11, 11, 9, 9, 9, 11, 9, 9, 9, 9, 9, 9, 7, 11, 9, 9, 9, 9, 11, 9, 9, 9, 9, 9, 7, 9, 11], [7, 9, 11, 11, 5, 9, 5, 1, 9, 5, 5, 5, 7, 5, 9, 15, 11, 5, 5, 5, 5, 5, 5, 5, 15, 9, 11, 5, 5, 6, 5, 5, 5, 15, 11, 5, 12, 2, 11, 5, 5, 11, 5, 5, 5, 7, 5, 5, 7, 7, 5, 5, 5, 5, 5, 11, 6, 6, 11, 6, 6, 11, 11, 15, 11, 9, 2, 5, 5, 11, 5, 5, 5, 7, 7, 5, 5, 5, 5, 6, 5, 11, 11, 5, 6, 6, 6, 6, 6, 6, 11, 2, 15, 15, 2, 11, 5, 5, 5, 11, 2, 5, 5, 6, 11, 2, 2, 5, 5, 5, 11, 11, 11, 11, 6, 6, 6, 11, 11, 11, 2, 2, 2, 11, 11, 2, 11, 11, 11, 11, 2, 11, 11, 6, 9, 5, 5, 0, 0, 15, 3, 5, 5, 5, 6, 3, 6, 6, 6, 6, 6, 9, 15, 0, 11, 11, 11, 11, 5, 5, 11, 7, 5, 5, 7, 7, 5, 5, 5, 5, 6, 5, 11, 7, 5, 6, 6, 6, 11, 6, 9, 6, 15, 6, 11, 9, 2, 11, 11, 11, 2, 2, 5, 5, 5, 5, 7, 6, 6, 6, 6, 6, 5, 5, 6, 5, 2, 5, 5, 5, 11, 6, 6, 6, 6, 6, 9, 2, 11, 11, 2, 11, 5, 5, 2, 2, 5, 11, 6, 6, 2, 11, 15, 6, 9, 2, 2, 5, 5, 5, 5, 2, 5, 5, 5, 7, 6, 5, 5, 5, 5, 5, 11, 11, 11, 5, 6, 6, 6, 6, 9, 6, 9, 2, 2, 11, 5, 5, 2, 2, 5, 5, 5, 6, 6, 6, 5, 6, 6, 9, 2, 5, 5, 6, 5, 11, 11, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 2, 2, 15, 9, 11, 2, 2, 11, 11, 5, 11, 9, 7, 9, 7, 7, 5, 6, 6, 6, 6, 9, 5, 9, 15, 2, 2, 5, 5, 3, 6, 2, 2, 11, 6, 6, 2, 2, 9, 6, 2, 2, 2, 5, 5, 6, 2, 6, 5, 6, 6, 6, 9, 2, 11, 11, 6, 9, 2, 11, 11, 9, 2, 2, 9, 2, 2, 5, 5, 6, 5, 5, 5, 11, 6, 5, 6, 6, 6, 9, 15, 2, 2, 2, 2, 2, 7, 2, 9, 5, 6, 6, 6, 11, 11, 9, 9, 6, 3, 6, 6, 9, 5, 15, 11, 11, 11, 11, 5, 2, 5, 11, 9, 5, 5, 7, 7, 7, 5, 5, 5, 7, 11, 5, 6, 6, 6, 6, 6, 6, 9, 2, 11, 11, 11, 11, 11, 5, 5, 5, 11, 11, 11, 5, 7, 9, 5, 5, 11, 9, 5, 5, 11, 7, 2, 11, 6, 9, 11, 11, 9, 6, 9, 6, 6, 6, 6, 11, 2, 2, 6, 11, 2, 2, 9, 5, 5, 2, 5, 5, 11, 7, 7, 9, 9, 2, 5, 5, 5, 5, 11, 11, 9, 6, 6, 6, 6, 9, 6, 11, 2, 9, 2, 2, 5, 5, 11, 2, 5, 5, 11, 7, 2, 2, 6], [7, 7, 7, 7, 8, 6, 6, 2, 9, 15, 3, 11, 8, 15, 12, 5, 7, 9, 15, 8, 12, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 15, 9, 9, 7, 9, 15, 15, 9, 15, 15, 15, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 2, 9, 9, 15, 7, 9, 15, 15, 15, 7, 7, 15, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 6, 3, 9, 9, 15, 7, 7, 15, 15, 15, 7, 7, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 7, 7, 7, 7, 7, 7, 9, 9, 15, 15, 7, 7, 7, 7, 7, 4, 6, 6, 15, 9, 15, 15, 7, 9, 15, 15, 15, 7, 7, 15, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 9, 15, 2, 9, 9, 9, 15, 15, 9, 0, 15, 15, 15, 15, 7, 7, 2, 7, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 15, 9, 15, 15, 15, 7, 7, 15, 7, 7, 15, 7, 7, 7, 2, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 15, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 15, 15, 7, 15, 7, 7, 7, 7, 2, 7, 7, 7, 7, 2, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 0, 7, 7, 7, 9, 7, 7, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 9, 9, 7, 9, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 7, 7, 7, 6, 3, 9, 9, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 9, 15, 9, 15, 7, 9, 15, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 9, 0, 15, 15, 7, 7, 7, 7, 7, 7, 9, 7, 9, 7, 7, 7, 7, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 15, 7, 15, 7, 7, 7, 7, 9, 7, 7, 7, 7]]\n",
      "True NER labels: [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 11, 0, 0, 0, 0, 0, 0, 15, 0, 11, 12, 12, 0, 5, 6, 6, 0, 0, 0, 5, 0, 0, 0, 0, 11, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 13, 14, 14, 14, 14, 0, 0, 0, 5, 6, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 7, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 15, 16, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 15, 16, 0, 5, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 15, 16, 0, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 7, 8, 0, 0, 0, 0, 0, 5, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7000\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         3\n",
      "    positive       0.70      1.00      0.82         7\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.35      0.50      0.41        10\n",
      "weighted avg       0.49      0.70      0.58        10\n",
      "\n",
      "\n",
      "NER Evaluation Debugging:\n",
      "Total Sentences Evaluated: 10\n",
      "First Prediction Example: ['I-gpe', 'I-gpe', 'B-geo', 'I-org', 'B-nat', 'B-org', 'B-geo', 'B-nat', 'B-org', 'B-nat', 'B-org', 'B-gpe', 'B-geo', 'B-geo', 'B-tim', 'B-nat', 'I-art', 'I-art', 'B-eve', 'B-nat', 'B-nat', 'I-nat', 'B-geo', 'B-nat', 'I-gpe', 'I-gpe', 'I-org']\n",
      "First Label Example: ['O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'B-geo', 'O', 'O']\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         art       0.00      0.00      0.00         0\n",
      "         eve       0.00      0.00      0.00         0\n",
      "         geo       0.00      0.00      0.00         7\n",
      "         gpe       0.00      0.00      0.00         4\n",
      "         nat       0.00      0.00      0.00         0\n",
      "         org       0.00      0.00      0.00         5\n",
      "         per       0.00      0.00      0.00         1\n",
      "         tim       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        21\n",
      "   macro avg       0.00      0.00      0.00        21\n",
      "weighted avg       0.00      0.00      0.00        21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86186\\AppData\\Roaming\\Python\\Python310\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\86186\\AppData\\Roaming\\Python\\Python310\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize multi-task model\n",
    "model = initialize_custom_model(pretrained_model, model_type=\"multitask\", pooling=\"mean\", num_classes=2,\n",
    "                                num_entity_labels=17)\n",
    "model.eval()\n",
    "\n",
    "# Load classification and NER datasets\n",
    "classification_file = \"data/raw/classification_dataset.xlsx\"\n",
    "ner_file = \"data/raw/ner_dataset.xlsx\"\n",
    "\n",
    "# set smaller sample ratio (0.1) for quick test, and larger sample ratio (1) for better performance\n",
    "#sample_ratio = 1\n",
    "sample_ratio = 0.1\n",
    "classification_train_loader, classification_test_loader, ner_train_loader, ner_test_loader = demo_data_loading(\n",
    "    classification_file, ner_file, sample_ratio)\n",
    "\n",
    "# Classification inference demo\n",
    "for batch_inputs, batch_labels in classification_test_loader:\n",
    "    with torch.no_grad():\n",
    "        classification_logits, _ = model(**batch_inputs)\n",
    "        predicted_classes = torch.argmax(classification_logits, dim=-1)\n",
    "        print(\"Predicted classes:\", predicted_classes.tolist())\n",
    "        print(\"True labels:\", batch_labels.tolist())\n",
    "    break\n",
    "\n",
    "# NER inference demo\n",
    "for batch_inputs, batch_labels in ner_test_loader:\n",
    "    with torch.no_grad():\n",
    "        _, ner_logits = model(**batch_inputs)\n",
    "        predicted_labels = torch.argmax(ner_logits, dim=-1)\n",
    "        print(\"Predicted NER labels:\", predicted_labels.tolist())\n",
    "        print(\"True NER labels:\", batch_labels.tolist())\n",
    "    break\n",
    "\n",
    "# Evaluate classification performance\n",
    "accuracy, report = evaluate_classification(model, classification_test_loader)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Report:\\n\", report)\n",
    "\n",
    "# Evaluate NER performance\n",
    "id_to_label = {v: k for k, v in ner_label_map.items()}\n",
    "ner_report = evaluate_ner(model, ner_test_loader, id_to_label)\n",
    "print(\"Report:\\n\", ner_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982ea6b",
   "metadata": {},
   "source": [
    "## 2.10 Explanation of Results\n",
    "The results could vary due to different random initilization.\n",
    "1. **Model Setup**:\n",
    "   - The model uses BERT’s setup (like `Vocab Size: 30522`, `Embedding Size: 768`). It copies weights from `bert-base-uncased` well. The message “Custom model initialized” shows everything worked fine at the start.\n",
    "\n",
    "2. **Loading Data**:\n",
    "   - The code uses saved files (`classification_dataset.pkl`, `ner_dataset.pkl`). It splits them into training and testing parts (e.g., Classification: 4989 train, 10 test; NER: 4775 train, 10 test). The test size is small because we use only 10% of data, good for a quick test but not enough for a big check.\n",
    "\n",
    "3. **Classification Testing** :\n",
    "   - The model predicts `[1, 0, 1, 0, 0, 0, 0, 0]` for classes, but true labels are `[1, 0, 0, 1, 0, 1, 0, 0]`. Some are correct (like first and second), but many are wrong. Since the model is not trained (only `eval()` mode), it gets 50% accuracy (0.5000).\n",
    "   - The report says precision is 0.50 for both negative and positive, but recall is 0.80 for negative and 0.20 for positive. This means it finds negative better but misses positive a lot. 50% accuracy is like guessing, so the model needs training to work better.\n",
    "\n",
    "4. **NER Testing**:\n",
    "   - Predicted NER labels (like `[1, 11, 7, 4, ...]`) become tags like `B-art`, `B-org`, `B-gpe`, but true labels are mostly `0` or `-100` (padding). For example, it predicts `B-art`, `O`, `I-eve`, but true is `B-geo`, `I-geo`. It gets the tags wrong or makes up new ones.\n",
    "   - The NER report shows all scores as 0.00, even with 18 real entities (like 7 `geo`, 3 `gpe`). This means it finds nothing right. The untrained NER part gives random answers, not matching the real data.\n",
    "\n",
    "5. **How the Model Works**:\n",
    "   - Without training, the model’s classification and NER parts don’t use BERT’s good pre-trained knowledge well. 50% accuracy in classification and 0.00 in NER show it acts random now.\n",
    "   - The BERT base is fine (weights copied right), but the new parts need training to fit the tasks. Training will make it learn the data and improve a lot.\n",
    "\n",
    "To sum up, the results show the model starts correctly with BERT, but without training, it does poorly—random for classification and nothing for NER. We must train it to make it good for these tasks, which is normal in NLP work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0fcfc",
   "metadata": {},
   "source": [
    "## 2.11 Summary\n",
    "This implementation is a robust and flexible multi-task learning setup:\n",
    "- **Data Handling**: Processes Excel data for classification and NER, with proper tokenization and label alignment.\n",
    "- **Model**: A `MultiTaskSentenceTransformer` with shared transformer layers and task-specific heads for efficiency.\n",
    "- **Evaluation**: Tailored metrics for classification (accuracy, F1) and NER (entity-level precision/recall).\n",
    "- **Usage**: Demonstrates loading, training readiness, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4351f7b",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a2ffc",
   "metadata": {},
   "source": [
    "Let’s move on to Task 3 and decide which method as the best one. Initially, we will first look at the limitations we have: regarding model size, hardware, we will evaluate three methods to freeze parts of the model, assess their advantages and disadvantages, and select the best training strategy. We have 50,000 samples for each dataset and are using an RTX 4060 laptop, and we will select a mandatory option while also come up with our preferred approach if given freedom to choose.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 1 Discussion of the 3 Training Options:\n",
    "\n",
    "\n",
    "### 1.1 Constraints to Consider\n",
    "\n",
    "Before the discussion, let's go through the limitations we are facing:\n",
    "\n",
    "- **Model Size**: The `bert-base-uncased` model has 110 million parameters, with 12 layers and embedding size being 768. The size of the two task specific heads are 1,536(sentiment analysis) and 6,912(ner) which are much smaller compared to the backbone. We will have 118 million parameters if fully unfrozen, which could be challenging considering our GPU.\n",
    "- **Hardware**: We have an RTX 4060 laptop GPU, the memory of is about 8GB. Training the entire model with 118 million parameters may exceed this capacity.\n",
    "- **Dataset Size**: We have 50,000 samples for each task. It should be enough for fine-tuning the backbone without excessive overfitting. This size appears adequate for the heads to learn, but the backbone’s 110 million parameters might require more data for optimal adjustment or we could apply a carefully chosen and smaller learning rate. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1.2 Option 1 Freezing the Whole Network\n",
    "\n",
    "#### Implications:\n",
    "- If we freeze everything—the transformer backbone and both heads—no weights update during training. We’d stick with the pre-trained BERT weights and random head weights.\n",
    "- With 50k samples per task, the model won’t learn anything specific, and the random heads will give us wild guesses. The backbone’s general knowledge won’t fit our tasks.\n",
    "- It’s like using the model as a static tool without improving it.\n",
    "\n",
    "#### Advantages:\n",
    "- Super fast since no updates are needed, which is nice on the RTX 4060 laptop if we’re just testing.\n",
    "- Gives a baseline to see how pre-trained embeddings do alone.\n",
    "\n",
    "#### Conclusion: not an ideal choice:\n",
    "This won’t work for training. The heads need to learn from our 50k datasets, and freezing everything stops that. It’s only good for quick tests, but not good for real training.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Option 2 Freezing Just the Transformer Backbone\n",
    "\n",
    "#### Implications:\n",
    "- The backbone (BERT layers) stays frozen with its pre-trained weights, while the classification and NER heads learn.\n",
    "- With 50k samples, the heads can adapt to our tasks, but the backbone won’t change. This might limit how well it fits our data, especially for NER’s detailed token needs.\n",
    "- Training is faster since we’re only updating the heads (a tiny fraction of the 110M+ parameters).\n",
    "\n",
    "#### Advantages:\n",
    "- Saves time and laptop power by skipping backbone updates, which fits the RTX 4060’s 8GB VRAM.\n",
    "- Keeps BERT’s general skills, avoiding overfitting on our 50k datasets.\n",
    "- Heads can still learn to handle classification and NER decently.\n",
    "\n",
    "#### Challenges:\n",
    "- The backbone’s embeddings might not be perfect for our tasks since they don’t adjust. \n",
    "- Performance might not be top-notch compared to training everything, especially with 50k data points that could benefit from fine-tuning.\n",
    "\n",
    "#### Conclusion: good pick\n",
    "This works well given our limitations. The RTX 4060 can handle training the heads (about 8,448 parameters) quickly, and 50k samples are enough for them to learn without overfitting. The backbone’s 110M parameters stay frozen, easing memory pressure, making this a practical choice for our hardware.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Option 3 Freezing One Task-Specific Head (Either Task A or Task B)\n",
    "\n",
    "#### What Happens (e.g., Freezing Classification, Training NER):\n",
    "- If we freeze one head (say, classification), it stays random. The other (NER) learns, and the backbone might tweak itself for NER.\n",
    "- With 50k samples, the trainable head can improve, but the frozen head stays useless. The backbone might focus too much on NER, hurting classification.\n",
    "- The model leans hard into the trainable task, which could throw things off balance.\n",
    "\n",
    "#### Implications:\n",
    "- The trainable head (e.g., NER) can get good with 50k data.\n",
    "- Saves some power by freezing one head.\n",
    "\n",
    "#### Challenges:\n",
    "- The frozen head (e.g., classification) will give random answers since it’s untrained.\n",
    "- The backbone might change too much for NER, making it bad for classification.\n",
    "- Training one task at a time gets messy and might need separate runs.\n",
    "\n",
    "#### Conclusion: not an ideal choice\n",
    "This isn’t great because both heads start random and need training with our 50k datasets. Freezing one leaves that task broken, which goes against doing both tasks together. It only makes sense if one head was pre-trained, which isn’t our case.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 How We Should Train the Model\n",
    "\n",
    "#### 1.5.1 If We Have to Pick One Option:\n",
    "- **Choice**: We’d go with **freezing just the transformer backbone (Option 2)**.\n",
    "- **Why?** The RTX 4060’s 8GB VRAM struggles with the 110M backbone parameters plus the heads’ 8,448 parameters during full training. Freezing the backbone limits updates to the heads, which is manageable with 50k samples per task. This avoids memory issues and lets the heads converge without overwhelming our laptop, making it the best forced choice.\n",
    "\n",
    "#### 1.5.2 If We Can Choose Freely:\n",
    "- **Choices**:\n",
    "  1. **Freeze Backbone, Train Heads**: Freeze backbone (110M params), train heads (~8,448 params). Low memory, keeps BERT’s knowledge, but limits NER. Good for small VRAM.\n",
    "  2. **Unfreeze Last Layers**: Train last 2-3 layers (~30M params) and heads, freeze early layers. Balances memory and tuning, misses some details. Good for medium needs.\n",
    "  3. **Unfreeze All**: Train all with adaptive rates (1e-6 early, 1e-4 heads). Best fit for 50k samples/task, high memory use, risks overfitting. Good for big tuning.\n",
    "- **Our Pick**: Freeze backbone, train heads. Why? RTX 4060’s 8GB VRAM can’t handle more. With 100k samples, this is simple and fast, using BERT’s embeddings well.\n",
    "- **How**: Batch size 8-16, learning rate 1e-4 for heads, dropout to stay stable.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 Summary\n",
    "\n",
    "Given our RTX 4060 laptop and 50k samples per task, **freezing the backbone** is the safest pick if we’re stuck with one option—it fits our hardware and lets the heads learn. But if we can choose and we have sufficient hardware resources, **training the whole model** is better, as 50k samples are enough to fine-tune the backbone without breaking our laptop, giving us the best shot at great results for both tasks with careful tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8bc40",
   "metadata": {},
   "source": [
    "## 2 Discussion of the Transfer Learning:\n",
    "---\n",
    "\n",
    "### 2.1 The Importance of Transfer Learning\n",
    "\n",
    "Transfer learning is really important when we use a BERT backbone because training a BERT model from scratch is extremely costly and unnecessary. It takes tons of time, data, and computing power—way more than my laptop, an RTX 4060, can handle! Instead, using a pre-trained model lets us skip that hard work. The pre-trained BERT already picks up general language knowledge from its huge pre-training corpus, like how words relate to each other. For our specific tasks—classification and NER—we just need small classifier heads to learn the domain-specific details, which is much easier and faster. This saves us a lot of trouble, especially on limited hardware.\n",
    "\n",
    "In Task 1, for testing purposes, we already shaped our model to match a pre-trained `bert-base-uncased` model and loaded its weights. This step proved our model works the same way, so now we can build on that for transfer learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Transfer Learning Strategy\n",
    "\n",
    "\n",
    "#### 2.2.1 Selection of Pre-Trained Model\n",
    "\n",
    "- **Model Choice**: I selected `bert-base-uncased` as the base.\n",
    "- **Reasoning**: \n",
    "  - `bert-base-uncased` has 12 layers, embedding size be 768, in total 110 million parameters, which balances well between performance and training availablily on consumer level GPU.\n",
    "  - Its uncased nature simplifies preprocessing by ignoring capitalization, and the WordPiece tokenizer effectively manages oov words.\n",
    "---\n",
    "\n",
    "#### 2.2.2 Layers to Freeze or Unfreeze\n",
    "\n",
    "We propose three unfreezing strategies as candidates for training. We could train the model with each and select the best based on experimental results if we have more hardware resources. For simplicity and test purposes we choose to test candidate 1.\n",
    "\n",
    "- **Candidate 1: Freeze the Backbone, Train the Heads**\n",
    "  - **Freeze**: Transformer backbone (110M parameters).\n",
    "  - **Unfreeze**: Classification head (~1,536 parameters) and NER head (~6,912 parameters).\n",
    "  - **Pros**: Reduces memory load on RTX 4060’s 8GB VRAM; preserves pre-trained knowledge; faster training.\n",
    "  - **Cons**: Backbone embeddings may not adapt, potentially limiting performance for NER.\n",
    "  - **Suitable Cases**: Limited resources; tasks closely aligned with pre-trained embeddings.\n",
    "\n",
    "- **Candidate 2: Unfreeze Only Backbone Layers Near the Output**\n",
    "  - **Unfreeze**: Last 2-3 backbone layers (e.g., layers 9-11) and heads.\n",
    "  - **Freeze**: Earlier layers (bottom 9-10).\n",
    "  - **Pros**: Lowers parameter load (~30M); fine-tunes task-specific layers; balances adaptability.\n",
    "  - **Cons**: Early layers stay static, possibly missing task nuances; moderate gains.\n",
    "  - **Suitable Cases**: Need balance between efficiency and adaptation; output layers need tuning.\n",
    "\n",
    "- **Candidate 3: Unfreeze the Entire Backbone with Adaptive Learning Rate**\n",
    "  - **Unfreeze**: All backbone layers and heads.\n",
    "  - **Learning Rate**: Adaptive—1e-6 for input layers, 1e-4 for output layers.\n",
    "  - **Pros**: Full fine-tuning adapts to 50,000 samples; adaptive rate reduces overfitting; best performance potential.\n",
    "  - **Cons**: High memory demand on RTX 4060; risk of overfitting if not tuned carefully.\n",
    "  - **Suitable Cases**: Sufficient data for fine-tuning; tasks need significant backbone adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.3 Rationale Behind These Choices\n",
    "\n",
    "- **Why Choose `bert-base-uncased`?**\n",
    "  - It is a reliable option due to its extensive pre-training on a vast corpus, providing a strong foundation. The uncased feature eliminates capitalization concerns, and its size is compatible with my laptop if I use small batches (e.g., 4-8). A sentiment-finetuned version enhances Task A from the start.\n",
    "\n",
    "- **Why Freeze the Backbone Initially?**\n",
    "  - The RTX 4060’s 8GB VRAM would struggle with 110 million backbone parameters plus the heads’ 8,448 parameters. Freezing the backbone enables the heads to learn with the 50,000 samples, aligning with memory limits and safeguarding pre-trained knowledge.\n",
    "\n",
    "- **Why Unfreeze Only Output Layers Later?**\n",
    "  - The output layers are critical for task-specific adjustments and can fine-tune with the 50,000 samples. Freezing the early layers reduces the parameter count, easing memory demands on the RTX 4060, and maintains broad language skills since our data is smaller than BERT’s pre-training corpus.\n",
    "\n",
    "- **Why Unfreeze the Entire Backbone with Adaptive Rates?**\n",
    "  - The 50,000 samples per task offer enough data to adjust the backbone without overfitting, and the adaptive learning rate—lower for input layers (1e-6) and higher for output layers (1e-4)—preserves early-layer general knowledge while tailoring later layers to our classification and NER tasks. This balances stability and improvement.\n",
    "\n",
    "- **Hardware and Dataset Fit**:\n",
    "  - The RTX 4060’s 8GB VRAM imposes constraints, so starting with a frozen backbone helps. Small batches or gradient accumulation can manage unfreezing later. The 50,000 samples per task are adequate for the heads and backbone to learn with careful tuning (e.g., low learning rates and dropout), avoiding overfitting.\n",
    "\n",
    "#### 2.2.4 Unfreeze stratege implementation\n",
    "\n",
    "The `set_requires_grad` function is designed to control which parameters of the model are trainable by setting the `requires_grad` attribute based on the chosen unfreezing strategy. It supports four options—`'heads_only'`, `'all_adaptive'`, `'output_layers'`, and `'all'`—allowing us to freeze or unfreeze specific layers of the transformer backbone and task-specific heads. By default, it freezes all parameters, then selectively unfreezes them according to the specified option, enabling flexible training configurations. This function is essential for experimenting with the three candidate approaches and the full unfreeze option, ensuring we can adapt the model efficiently within our hardware constraints and evaluate the best strategy based on experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2989b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set requires_grad for specific layers\n",
    "def set_requires_grad(model, unfreeze_option=None):\n",
    "    \"\"\"\n",
    "    Set requires_grad for model parameters based on unfreeze_option.\n",
    "    Options: 'heads_only', 'all_adaptive', 'output_layers', 'all'\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  \n",
    "    if unfreeze_option == 'heads_only':\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classification_head' in name or 'ner_head' in name:\n",
    "                param.requires_grad = True\n",
    "    elif unfreeze_option == 'all_adaptive':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    elif unfreeze_option == 'output_layers':\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'encoder_layers.9' in name or 'encoder_layers.10' in name or 'encoder_layers.11' in name or \\\n",
    "               'classification_head' in name or 'ner_head' in name:\n",
    "                param.requires_grad = True\n",
    "    elif unfreeze_option == 'all':\n",
    "        # Unfreeze all layers\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "def demo_unfreeze_strategy(model):\n",
    "    \"\"\"\n",
    "    Demonstrates different unfreeze strategies and prints the number of trainable parameters.\n",
    "    \"\"\"\n",
    "    strategies = ['heads_only', 'all_adaptive', 'output_layers', 'all']\n",
    "\n",
    "    for strategy in strategies:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Applying Unfreeze Strategy: {strategy}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Apply the unfreeze strategy\n",
    "        set_requires_grad(model, strategy)\n",
    "\n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        print(f\" Trainable Parameters: {trainable_params:,} / {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3269c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Applying Unfreeze Strategy: heads_only\n",
      "==================================================\n",
      " Trainable Parameters: 14,611 / 108,906,259\n",
      "\n",
      "==================================================\n",
      "Applying Unfreeze Strategy: all_adaptive\n",
      "==================================================\n",
      " Trainable Parameters: 108,906,259 / 108,906,259\n",
      "\n",
      "==================================================\n",
      "Applying Unfreeze Strategy: output_layers\n",
      "==================================================\n",
      " Trainable Parameters: 21,278,227 / 108,906,259\n",
      "\n",
      "==================================================\n",
      "Applying Unfreeze Strategy: all\n",
      "==================================================\n",
      " Trainable Parameters: 108,906,259 / 108,906,259\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate different unfreeze strategies\n",
    "demo_unfreeze_strategy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd9795",
   "metadata": {},
   "source": [
    "### 2.3 Explanation of Results\n",
    "\n",
    "The results show trainable parameters out of 108,906,259 total:\n",
    "\n",
    "1. **Heads Only**: 14,611  \n",
    "   - Only heads train. Low memory, good for RTX 4060, but backbone stays fixed.\n",
    "2. **All Adaptive**: 108,906,259  \n",
    "   - All layers train. High memory, best fit, but tough on 8GB VRAM.\n",
    "3. **Output Layers**: 21,278,227  \n",
    "   - Last 3 layers and heads train. Balances memory and tuning.\n",
    "4. **All**: 108,906,259  \n",
    "   - Everything trains. Same as “all adaptive,” heavy for RTX 4060.\n",
    "\n",
    "**Meaning**: “Heads only” fits our hardware best (1.5.2 pick). “Output layers” adds more but needs care. “All” and “all adaptive” are too big. Numbers prove the code sets layers right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f34566",
   "metadata": {},
   "source": [
    "### 2.4 Summary of Key Decisions and Insights  for Task 3\n",
    "\n",
    "- **Training Choice**: I chose to **freeze the backbone and train the heads**—best for both forced and free scenarios. My RTX 4060’s 8GB VRAM can’t handle all 118M parameters (110M backbone + 8,448 heads), and 50k samples per task (100k total) suit the heads well.\n",
    "- **Rejected Options**: Freezing all stops learning—useless with 50k samples. Freezing one head unbalances tasks—both need training.\n",
    "- **Free Preference**: I stuck with freezing the backbone over unfreezing last layers (~30M) or all (118M). VRAM limits favor simplicity; 50k samples don’t need full tuning.\n",
    "- **Model**: Picked `bert-base-uncased` (110M params, 12 layers)—pre-trained, fits my laptop with small batches (8-16).\n",
    "- **Layer Plan**: Freeze backbone, train heads (~8,448 params)—saves VRAM, uses BERT’s skills, adapts fast with 50k samples.\n",
    "- **How**: Batch size 8-16, 1e-4 rate for heads, dropout for stability.\n",
    "- **Insights**: 8GB VRAM limits me to small training; 50k samples work for heads, not full backbone. Transfer learning with BERT saves effort—heads focus on tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2bec4",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation\n",
    "\n",
    "Finally we have got to Task 4, to implement a training loop for our `MultiTaskSentenceTransformer` model, which performs classificationand NER. We will use datasets, data loaders, and model structure defined in Task 2. This introduction will analyze the task requirements, explain our decisions, and highlight the components we need to implement, preparing you to read the code.\n",
    "\n",
    "---\n",
    "\n",
    "## 1 Task Analysis and Approach\n",
    "\n",
    "### 1.1 Task Analysis\n",
    "Task 4 requires us to implement a training loop for our multi-task learning (MTL) model, focusing on handling hypothetical data, the forward pass, and metrics tracking. The model must optimize both tasks—classification and NER—using the shared transformer backbone, with separate train and test phases to compare performance before and after training. The MTL framework necessitates careful handling of dual objectives, ensuring both heads (classification and NER) learn effectively without interference, while managing our hardware constraints (RTX 4060 with 8GB VRAM).\n",
    "\n",
    "### 1.2 Decisions Made\n",
    "- **Freezing the Backbone**: For simplicity, we will freeze the transformer backbone (110 million parameters) and train only the classification head (~1,536 parameters) and NER head (~6,912 parameters). This reduces memory usage, making training feasible on our RTX 4060, while focusing on task-specific learning in the heads.\n",
    "- **Training Mode**: We will implement a flexible training loop supporting both **combined training** (processing both tasks together) and **separate training** (optimizing tasks independently), controlled by a `combine_tasks` parameter. Combined training leverages MTL benefits by optimizing both heads simultaneously, but requires loss balancing. Separate training avoids this complexity, focusing on each task individually, which is simpler given the frozen backbone.\n",
    "- **Loss Weighting**: In combined mode, we’ll weight classification and NER losses equally (0.5 each) for simplicity, acknowledging that NER might produce larger losses due to its complexity.\n",
    "- **Metrics**: We’ll track accuracy and a classification report for Task A, and an entity-level F1-report for Task B, using the existing `evaluate_classification` and `evaluate_ner` functions from Task 2. Metrics will be computed before and after training on test sets to assess improvement.\n",
    "\n",
    "### 1.3 How We’ll Implement It\n",
    "- **Handling the Dataset**: We already have implemented `classification_loader` and `ner_loader`. To train both tasks together, we’ll set up a `CombinedTaskIterator`, which will switch between classification and NER batches without changing `MultiTaskDataset` so that we keep things organized without changing with the core dataset structure.\n",
    "- **Forward Pass**: The model will compute `classification_logits` and `ner_logits` for each batch. In combined mode, both losses are calculated and combined; in separate mode, only the relevant loss is computed per task.\n",
    "- **Training Loop**: We’ll implement a `train_and_evaluate` function that supports both modes. It will freeze the backbone, train the heads, print losses per epoch, and evaluate metrics before and after training on test sets.\n",
    "- **Evaluation**: Metrics will be printed for both tasks, allowing comparison of performance (e.g., accuracy for classification, F1 for NER) to confirm the heads’ learning progress.\n",
    "\n",
    "### 1.4 Components to Implement\n",
    "- **CombinedTaskIterator**: A wrapper to pair classification and NER batches for combined training, ensuring balanced sampling without altering `MultiTaskDataset`.\n",
    "- **Freeze Backbone Function**: A utility to freeze the backbone and unfreeze the heads, ensuring only the heads’ parameters are updated.\n",
    "- **Training and Evaluation Loop**: The main loop will handle both training modes, compute losses, and evaluate metrics using existing functions, with separate train and test phases.\n",
    "\n",
    "With these decisions and components in place, we’re ready to implement the training loop, providing flexibility to experiment with both combined and separate training approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b75de",
   "metadata": {},
   "source": [
    "## 2 Explanation of Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6357a9",
   "metadata": {},
   "source": [
    "### 2.1 Imports\n",
    "\n",
    "- Imports essential libraries (**PyTorch**, **tqdm**, **sklearn**, **seqeval**) for training, evaluation, and metrics in classification and NER tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9bd0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.scheme import IOB2\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5e979",
   "metadata": {},
   "source": [
    "### 2.2 Combining Classification and NER Batches\n",
    "- Wraps **classification** and **NER** data loaders into a unified iterator.\n",
    "- Cycles through batches seamlessly, resetting iterators on exhaustion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb17b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to combine classification and NER batches\n",
    "class CombinedTaskIterator:\n",
    "    def __init__(self, classification_loader, ner_loader):\n",
    "        self.classification_loader = classification_loader\n",
    "        self.ner_loader = ner_loader\n",
    "        self.class_iter = iter(classification_loader)\n",
    "        self.ner_iter = iter(ner_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            class_batch = next(self.class_iter)\n",
    "        except StopIteration:\n",
    "            self.class_iter = iter(self.classification_loader)\n",
    "            class_batch = next(self.class_iter)\n",
    "        \n",
    "        try:\n",
    "            ner_batch = next(self.ner_iter)\n",
    "        except StopIteration:\n",
    "            self.ner_iter = iter(self.ner_loader)\n",
    "            ner_batch = next(self.ner_iter)\n",
    "        \n",
    "        return {\"classification\": class_batch, \"ner\": ner_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb153515",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Task Model Training and Evaluation\n",
    "- Freezes the model backbone with `freeze_backbone()`, keeping **classification** and **NER** heads trainable.\n",
    "- Trains and evaluates a multi-task model in **combined** or **separate** modes, using weighted loss and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "330155a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone while keeping output heads trainable\n",
    "def freeze_backbone(model):\n",
    "    \"\"\"Freezes model backbone while allowing classification & NER heads to be trainable.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = ('classification_head' in name or 'ner_head' in name)\n",
    "\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, classification_train_loader, ner_train_loader,\n",
    "                       classification_test_loader, ner_test_loader,\n",
    "                       criterion_classification, criterion_ner,\n",
    "                       optimizer, device, id_to_label, combine_tasks=False, epochs=3, loss_weight=0.5):\n",
    "    \"\"\"Trains and evaluates the multi-task model in either combined or separate mode.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Evaluate before training\n",
    "    print(\"\\nMetrics Before Training:\")\n",
    "    print(\"Classification:\")\n",
    "    accuracy_before, report_before = evaluate_classification(model, classification_test_loader, device)\n",
    "    print(f\"Accuracy: {accuracy_before:.4f}\")\n",
    "    print(\"Report:\\n\", report_before)\n",
    "\n",
    "    print(\"NER:\")\n",
    "    ner_report_before = evaluate_ner(model, ner_test_loader, id_to_label, device)\n",
    "    print(\"Report:\\n\", ner_report_before)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Combined training mode\n",
    "    if combine_tasks:\n",
    "        print(\"\\nTraining in Combined Mode...\")\n",
    "        combined_iterator = CombinedTaskIterator(classification_train_loader, ner_train_loader)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, batch in enumerate(tqdm(combined_iterator, desc=f\"Combined Epoch {epoch + 1}/{epochs}\",\n",
    "                                           total=len(classification_train_loader))):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Classification batch\n",
    "                class_inputs, class_labels = batch[\"classification\"]\n",
    "                class_inputs = {k: v.to(device) for k, v in class_inputs.items()}\n",
    "                class_labels = class_labels.to(device)\n",
    "                class_logits, _ = model(**class_inputs)\n",
    "                loss_class = criterion_classification(class_logits, class_labels)\n",
    "\n",
    "                # NER batch\n",
    "                ner_inputs, ner_labels = batch[\"ner\"]\n",
    "                ner_inputs = {k: v.to(device) for k, v in ner_inputs.items()}\n",
    "                ner_labels = ner_labels.to(device)\n",
    "                _, ner_logits = model(**ner_inputs)\n",
    "                loss_ner = criterion_ner(ner_logits.view(-1, ner_logits.size(-1)), ner_labels.view(-1))\n",
    "\n",
    "                # Combined loss\n",
    "                loss = loss_weight * loss_class + (1 - loss_weight) * loss_ner\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"\\nCombined Epoch {epoch + 1} Average Loss: {total_loss / len(classification_train_loader):.4f}\")\n",
    "\n",
    "    # Separate training mode\n",
    "    else:\n",
    "        print(\"\\nTraining in Separate Mode...\")\n",
    "\n",
    "        # Train Classification\n",
    "        print(\"Training Classification Head...\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_inputs, batch_labels in tqdm(classification_train_loader, desc=f\"Classification Epoch {epoch + 1}/{epochs}\"):\n",
    "                batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                class_logits, _ = model(**batch_inputs)\n",
    "                loss = criterion_classification(class_logits, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"\\nClassification Epoch {epoch + 1} Average Loss: {total_loss / len(classification_train_loader):.4f}\")\n",
    "\n",
    "        # Train NER\n",
    "        print(\"\\nTraining NER Head...\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for batch_inputs, batch_labels in tqdm(ner_train_loader, desc=f\"NER Epoch {epoch + 1}/{epochs}\"):\n",
    "                batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                _, ner_logits = model(**batch_inputs)\n",
    "                loss = criterion_ner(ner_logits.view(-1, ner_logits.size(-1)), batch_labels.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"\\nNER Epoch {epoch + 1} Average Loss: {total_loss / len(ner_train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate after training\n",
    "    print(\"\\nMetrics After Training:\")\n",
    "    print(\"Classification:\")\n",
    "    accuracy_after, report_after = evaluate_classification(model, classification_test_loader)\n",
    "    print(f\"Accuracy: {accuracy_after:.4f}\")\n",
    "    print(\"Report:\\n\", report_after)\n",
    "\n",
    "    print(\"NER:\")\n",
    "    ner_report_after = evaluate_ner(model, ner_test_loader, id_to_label)\n",
    "    print(\"Report:\\n\", ner_report_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748c39a",
   "metadata": {},
   "source": [
    "### 2.4 Multi-Task Model Training and Evaluation\n",
    "- Sets up the **device** (GPU/CPU), freezes the backbone, and defines **loss functions** and **optimizer**.\n",
    "- Trains the model with **separate task training** for classification and NER using `train_and_evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3409bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Separate Approach...\n",
      "\n",
      "Metrics Before Training:\n",
      "Classification:\n",
      "Accuracy: 0.5000\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.80      0.62         5\n",
      "    positive       0.50      0.20      0.29         5\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.50      0.50      0.45        10\n",
      "weighted avg       0.50      0.50      0.45        10\n",
      "\n",
      "NER:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86186\\AppData\\Roaming\\Python\\Python310\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Evaluation Debugging:\n",
      "Total Sentences Evaluated: 10\n",
      "First Prediction Example: ['B-gpe', 'B-gpe', 'I-eve', 'I-eve', 'I-eve', 'I-eve', 'I-eve', 'O', 'I-per', 'I-eve', 'I-eve', 'B-org']\n",
      "First Label Example: ['O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'B-gpe', 'O', 'O']\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         art       0.00      0.00      0.00         0\n",
      "         eve       0.00      0.00      0.00         0\n",
      "         geo       0.00      0.00      0.00         7\n",
      "         gpe       0.00      0.00      0.00         3\n",
      "         nat       0.00      0.00      0.00         0\n",
      "         org       0.00      0.00      0.00         2\n",
      "         per       0.00      0.00      0.00         0\n",
      "         tim       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        18\n",
      "   macro avg       0.00      0.00      0.00        18\n",
      "weighted avg       0.00      0.00      0.00        18\n",
      "\n",
      "\n",
      "Training in Separate Mode...\n",
      "Training Classification Head...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classification Epoch 1/3: 100%|██████████████████████████████████████████████████████| 624/624 [02:07<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Epoch 1 Average Loss: 0.5158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classification Epoch 2/3: 100%|██████████████████████████████████████████████████████| 624/624 [02:15<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Epoch 2 Average Loss: 0.3910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classification Epoch 3/3: 100%|██████████████████████████████████████████████████████| 624/624 [02:20<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Epoch 3 Average Loss: 0.3544\n",
      "\n",
      "Training NER Head...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER Epoch 1/3: 100%|█████████████████████████████████████████████████████████████████| 597/597 [02:13<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Epoch 1 Average Loss: 0.5309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER Epoch 2/3: 100%|█████████████████████████████████████████████████████████████████| 597/597 [02:12<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Epoch 2 Average Loss: 0.2918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NER Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████| 597/597 [02:11<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Epoch 3 Average Loss: 0.2601\n",
      "\n",
      "Metrics After Training:\n",
      "Classification:\n",
      "Accuracy: 0.9000\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      1.00      0.91         5\n",
      "    positive       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.92      0.90      0.90        10\n",
      "weighted avg       0.92      0.90      0.90        10\n",
      "\n",
      "NER:\n",
      "\n",
      "NER Evaluation Debugging:\n",
      "Total Sentences Evaluated: 10\n",
      "First Prediction Example: ['B-geo', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O']\n",
      "First Label Example: ['B-geo', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'O']\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.50      0.57      0.53         7\n",
      "         gpe       0.75      1.00      0.86         3\n",
      "         org       0.00      0.00      0.00         2\n",
      "         tim       0.83      0.83      0.83         6\n",
      "\n",
      "   micro avg       0.63      0.67      0.65        18\n",
      "   macro avg       0.52      0.60      0.56        18\n",
      "weighted avg       0.60      0.67      0.63        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Freeze backbone layers while keeping classification & NER heads trainable\n",
    "freeze_backbone(model)\n",
    "\n",
    "# Define loss functions for classification and NER\n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_ner = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Define optimizer (only update trainable parameters)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "# ID-to-label mapping for NER evaluation\n",
    "id_to_label = {v: k for k, v in ner_label_map.items()}\n",
    "\n",
    "# Train model with separate task training\n",
    "print(\"Training with Separate Approach...\")\n",
    "train_and_evaluate(model, classification_train_loader, ner_train_loader,\n",
    "                   classification_test_loader, ner_test_loader,\n",
    "                   criterion_classification, criterion_ner,\n",
    "                   optimizer, device, id_to_label,\n",
    "                   combine_tasks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df1dae",
   "metadata": {},
   "source": [
    "### 2.5 Explanation of Results\n",
    "The output shows training and evaluation on an RTX 4060 with the backbone frozen and heads trained separately using 10% of the data (50k samples per task, sampled to 5k train/10 test).\n",
    "\n",
    "#### Before Training\n",
    "- **Classification**: Accuracy is 0.5000 (random guessing). Precision/recall/F1 are low (e.g., 0.50/0.80/0.62 for negative), showing the untrained heads don’t work yet.\n",
    "- **NER**: All metrics are 0.00 (no correct predictions). Predictions like `B-gpe`, `I-eve` mismatch true labels (`O`, `B-geo`), confirming the NER head is random before training.\n",
    "\n",
    "#### Training Process\n",
    "- **Classification**: Loss drops from 0.5158 (Epoch 1) to 0.3544 (Epoch 3) over 624 batches per epoch. It’s steady progress with 5k samples.\n",
    "- **NER**: Loss reduces from 0.5309 (Epoch 1) to 0.2601 (Epoch 3) over 597 batches. The decrease shows learning despite the small dataset.\n",
    "\n",
    "#### After Training\n",
    "- **Classification**: Accuracy jumps to 0.9000. Precision/recall/F1 improve (e.g., 0.83/1.00/0.91 for negative, 1.00/0.80/0.89 for positive), showing strong learning on 10 samples.\n",
    "- **NER**: Metrics rise—micro F1 is 0.65 (from 0.00). Predictions align better (e.g., `B-geo`, `O`, `B-tim` match true labels), with F1 scores like 0.53 (geo) and 0.86 (gpe). Some entities (org) still lag.\n",
    "\n",
    "#### Key Point\n",
    "Although we only use 10% of the data, the model converges well. Loss reduces steadily (0.5158 to 0.3544 for classification, 0.5309 to 0.2601 for NER), and metrics improve (accuracy 0.50 to 0.90, NER F1 0.00 to 0.65). This shows our model works and trains as expected, even with a small sample, proving the heads learn effectively with the frozen backbone on RTX 4060."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a34e8",
   "metadata": {},
   "source": [
    "### 2.6 Summary of Key Decisions and Insights for Task 4\n",
    "- **Training Strategy**: I decided to **freeze the backbone** (110M parameters) and train only the heads (~1,536 for classification, ~6,912 for NER). This keeps memory low for my RTX 4060’s 8GB VRAM, making training possible, and focuses learning on the task-specific heads.\n",
    "- **Training Mode**: I chose a **flexible loop** with two options—**combined training** (both tasks together) and **separate training** (tasks alone), controlled by `combine_tasks`. Combined mode mixes tasks with equal loss weights (0.5 each) for MTL benefits, while separate mode simplifies by training each head on its own, suiting the frozen backbone.\n",
    "- **Implementation Choices**: I used a `CombinedTaskIterator` to pair batches for combined mode without changing `MultiTaskDataset`, keeping things neat. The forward pass computes both `classification_logits` and `ner_logits`, with losses handled per mode. The `train_and_evaluate` function freezes the backbone, trains heads, tracks losses, and shows metrics before and after.\n",
    "- **Metrics Tracking**: I picked **accuracy** and a report for classification, and **F1-score** for NER, using existing functions. This compares performance on test sets to confirm the heads improve.\n",
    "- **Insights**: Freezing the backbone fits my hardware limits and works with 50k samples per task. Combined training could boost both tasks but needs careful loss balancing (NER losses might dominate). Separate training is simpler and safer for now. The setup lets me test both approaches easily, showing how well the heads learn with a fixed backbone.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
